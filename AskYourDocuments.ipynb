{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“„ AskYourDocuments: Chat with Your Documents using AI\n",
        "\n",
        "Welcome to **AskYourDocuments**! This notebook creates an intelligent document analysis system that lets you upload various document types (PDF, DOCX, XLSX, PPTX, images) and ask questions about their content using powerful AI models.\n",
        "\n",
        "## ğŸ” What This Notebook Does\n",
        "\n",
        "1. Sets up a complete document processing pipeline\n",
        "2. Extracts text, tables, and image content from your documents\n",
        "3. Creates a searchable knowledge base from your documents\n",
        "4. Deploys a web interface to upload documents and ask questions\n",
        "5. Processes your queries using AI and retrieves targeted information\n",
        "\n",
        "## ğŸ“‹ How To Use This Notebook (in Google Colab)\n",
        "\n",
        "1. **Run each cell in sequence** from top to bottom\n",
        "2. **Set up your API keys** in the second cell:\n",
        "   - Azure API key - for embeddings and LLM (required)\n",
        "   - Hugging Face API token - for vision features (optional)\n",
        "   - Ngrok authtoken - for better web access (recommended)\n",
        "3. **Upload HTML and JS files** when prompted (or use the auto-generated basic templates)\n",
        "4. The **final cell launches the web interface** via ngrok link\n",
        "\n",
        "## ğŸ”‘ Required API Keys\n",
        "\n",
        "- **Azure AI Inference API Key**: Required for document processing and query answering. Get one from your [Azure AI account](https://github.com/settings/tokens).\n",
        "- **Hugging Face API Token**: Optional for enhanced vision features. Get a token from your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n",
        "- **Ngrok Authtoken**: Recommended for stable public URLs. Sign up at [ngrok.com](https://ngrok.com/) and get your token.\n",
        "\n",
        "## ğŸš€ Getting Started\n",
        "\n",
        "Run the first cell below to install all required dependencies, then proceed through each cell sequentially. Make sure to configure your API keys in the second cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uLVtE7_b9rW",
        "outputId": "b1e47df5-3560-427c-e227-4abfa576e801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 127.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-opensymbol.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../000-fonts-opensymbol_2%3a102.12+LibO7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking fonts-opensymbol (2:102.12+LibO7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-style-colibre.\n",
            "Preparing to unpack .../001-libreoffice-style-colibre_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-style-colibre (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libuno-sal3.\n",
            "Preparing to unpack .../002-libuno-sal3_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libuno-sal3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libuno-salhelpergcc3-3.\n",
            "Preparing to unpack .../003-libuno-salhelpergcc3-3_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libuno-salhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libuno-cppu3.\n",
            "Preparing to unpack .../004-libuno-cppu3_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libuno-cppu3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package uno-libs-private.\n",
            "Preparing to unpack .../005-uno-libs-private_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking uno-libs-private (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package liblangtag-common.\n",
            "Preparing to unpack .../006-liblangtag-common_0.6.3-2ubuntu1_all.deb ...\n",
            "Unpacking liblangtag-common (0.6.3-2ubuntu1) ...\n",
            "Selecting previously unselected package liblangtag1:amd64.\n",
            "Preparing to unpack .../007-liblangtag1_0.6.3-2ubuntu1_amd64.deb ...\n",
            "Unpacking liblangtag1:amd64 (0.6.3-2ubuntu1) ...\n",
            "Selecting previously unselected package libuno-cppuhelpergcc3-3.\n",
            "Preparing to unpack .../008-libuno-cppuhelpergcc3-3_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libuno-cppuhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libuno-purpenvhelpergcc3-3.\n",
            "Preparing to unpack .../009-libuno-purpenvhelpergcc3-3_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libuno-purpenvhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package ure.\n",
            "Preparing to unpack .../010-ure_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking ure (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-common.\n",
            "Preparing to unpack .../011-libreoffice-common_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-common (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libabsl20210324:amd64.\n",
            "Preparing to unpack .../012-libabsl20210324_0~20210324.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libabsl20210324:amd64 (0~20210324.2-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libclucene-core1v5:amd64.\n",
            "Preparing to unpack .../013-libclucene-core1v5_2.3.3.4+dfsg-1ubuntu5_amd64.deb ...\n",
            "Unpacking libclucene-core1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Selecting previously unselected package libclucene-contribs1v5:amd64.\n",
            "Preparing to unpack .../014-libclucene-contribs1v5_2.3.3.4+dfsg-1ubuntu5_amd64.deb ...\n",
            "Unpacking libclucene-contribs1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Selecting previously unselected package libeot0:amd64.\n",
            "Preparing to unpack .../015-libeot0_0.01-5build2_amd64.deb ...\n",
            "Unpacking libeot0:amd64 (0.01-5build2) ...\n",
            "Selecting previously unselected package libexttextcat-data.\n",
            "Preparing to unpack .../016-libexttextcat-data_3.4.5-1build2_all.deb ...\n",
            "Unpacking libexttextcat-data (3.4.5-1build2) ...\n",
            "Selecting previously unselected package libexttextcat-2.0-0:amd64.\n",
            "Preparing to unpack .../017-libexttextcat-2.0-0_3.4.5-1build2_amd64.deb ...\n",
            "Unpacking libexttextcat-2.0-0:amd64 (3.4.5-1build2) ...\n",
            "Selecting previously unselected package libgpgme11:amd64.\n",
            "Preparing to unpack .../018-libgpgme11_1.16.0-1.2ubuntu4.2_amd64.deb ...\n",
            "Unpacking libgpgme11:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Selecting previously unselected package libgpgmepp6:amd64.\n",
            "Preparing to unpack .../019-libgpgmepp6_1.16.0-1.2ubuntu4.2_amd64.deb ...\n",
            "Unpacking libgpgmepp6:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Selecting previously unselected package libharfbuzz-icu0:amd64.\n",
            "Preparing to unpack .../020-libharfbuzz-icu0_2.7.4-1ubuntu3.2_amd64.deb ...\n",
            "Unpacking libharfbuzz-icu0:amd64 (2.7.4-1ubuntu3.2) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../021-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libhyphen0:amd64.\n",
            "Preparing to unpack .../022-libhyphen0_2.8.8-7build2_amd64.deb ...\n",
            "Unpacking libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Selecting previously unselected package libmythes-1.2-0:amd64.\n",
            "Preparing to unpack .../023-libmythes-1.2-0_2%3a1.2.4-4build1_amd64.deb ...\n",
            "Unpacking libmythes-1.2-0:amd64 (2:1.2.4-4build1) ...\n",
            "Selecting previously unselected package liborcus-parser-0.17-0:amd64.\n",
            "Preparing to unpack .../024-liborcus-parser-0.17-0_0.17.2-2_amd64.deb ...\n",
            "Unpacking liborcus-parser-0.17-0:amd64 (0.17.2-2) ...\n",
            "Selecting previously unselected package liborcus-0.17-0:amd64.\n",
            "Preparing to unpack .../025-liborcus-0.17-0_0.17.2-2_amd64.deb ...\n",
            "Unpacking liborcus-0.17-0:amd64 (0.17.2-2) ...\n",
            "Selecting previously unselected package libyajl2:amd64.\n",
            "Preparing to unpack .../026-libyajl2_2.1.0-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libyajl2:amd64 (2.1.0-3ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libraptor2-0:amd64.\n",
            "Preparing to unpack .../027-libraptor2-0_2.0.15-0ubuntu4.1_amd64.deb ...\n",
            "Unpacking libraptor2-0:amd64 (2.0.15-0ubuntu4.1) ...\n",
            "Selecting previously unselected package libmhash2:amd64.\n",
            "Preparing to unpack .../028-libmhash2_0.9.9.9-9build2_amd64.deb ...\n",
            "Unpacking libmhash2:amd64 (0.9.9.9-9build2) ...\n",
            "Selecting previously unselected package librasqal3:amd64.\n",
            "Preparing to unpack .../029-librasqal3_0.9.33-0.2ubuntu1_amd64.deb ...\n",
            "Unpacking librasqal3:amd64 (0.9.33-0.2ubuntu1) ...\n",
            "Selecting previously unselected package librdf0:amd64.\n",
            "Preparing to unpack .../030-librdf0_1.0.17-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking librdf0:amd64 (1.0.17-1.1ubuntu3) ...\n",
            "Selecting previously unselected package librevenge-0.0-0:amd64.\n",
            "Preparing to unpack .../031-librevenge-0.0-0_0.0.4-6ubuntu7_amd64.deb ...\n",
            "Unpacking librevenge-0.0-0:amd64 (0.0.4-6ubuntu7) ...\n",
            "Selecting previously unselected package libxmlsec1:amd64.\n",
            "Preparing to unpack .../032-libxmlsec1_1.2.33-1build2_amd64.deb ...\n",
            "Unpacking libxmlsec1:amd64 (1.2.33-1build2) ...\n",
            "Selecting previously unselected package libxmlsec1-nss:amd64.\n",
            "Preparing to unpack .../033-libxmlsec1-nss_1.2.33-1build2_amd64.deb ...\n",
            "Unpacking libxmlsec1-nss:amd64 (1.2.33-1build2) ...\n",
            "Selecting previously unselected package libreoffice-core.\n",
            "Preparing to unpack .../034-libreoffice-core_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-core (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-base-core.\n",
            "Preparing to unpack .../035-libreoffice-base-core_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-base-core (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-base-drivers.\n",
            "Preparing to unpack .../036-libreoffice-base-drivers_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-base-drivers (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-base.\n",
            "Preparing to unpack .../037-libreoffice-base_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "No diversion 'diversion of /usr/lib/libreoffice/share/basic/dialog.xlc to /usr/lib/libreoffice/share/basic/dialog.xlc.noaccess by libreoffice-base', none removed.\n",
            "No diversion 'diversion of /usr/lib/libreoffice/share/basic/script.xlc to /usr/lib/libreoffice/share/basic/script.xlc.noaccess by libreoffice-base', none removed.\n",
            "Unpacking libreoffice-base (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../038-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "Preparing to unpack .../039-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package apparmor.\n",
            "Preparing to unpack .../040-apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../041-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../042-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../043-openjdk-11-jre_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../044-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../045-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package firebird3.0-common-doc.\n",
            "Preparing to unpack .../046-firebird3.0-common-doc_3.0.8.33535.ds4-1ubuntu2_all.deb ...\n",
            "Unpacking firebird3.0-common-doc (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package firebird3.0-common.\n",
            "Preparing to unpack .../047-firebird3.0-common_3.0.8.33535.ds4-1ubuntu2_all.deb ...\n",
            "Unpacking firebird3.0-common (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package libtommath1:amd64.\n",
            "Preparing to unpack .../048-libtommath1_1.2.0-6ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libtommath1:amd64 (1.2.0-6ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libfbclient2:amd64.\n",
            "Preparing to unpack .../049-libfbclient2_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking libfbclient2:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package libib-util:amd64.\n",
            "Preparing to unpack .../050-libib-util_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking libib-util:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package firebird3.0-server-core:amd64.\n",
            "Preparing to unpack .../051-firebird3.0-server-core_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking firebird3.0-server-core:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package firebird3.0-utils.\n",
            "Preparing to unpack .../052-firebird3.0-utils_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking firebird3.0-utils (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package fonts-crosextra-caladea.\n",
            "Preparing to unpack .../053-fonts-crosextra-caladea_20130214-2.1_all.deb ...\n",
            "Unpacking fonts-crosextra-caladea (20130214-2.1) ...\n",
            "Selecting previously unselected package fonts-crosextra-carlito.\n",
            "Preparing to unpack .../054-fonts-crosextra-carlito_20130920-1.1_all.deb ...\n",
            "Unpacking fonts-crosextra-carlito (20130920-1.1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../055-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../056-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu.\n",
            "Preparing to unpack .../057-fonts-dejavu_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-liberation2.\n",
            "Preparing to unpack .../058-fonts-liberation2_2.1.5-1_all.deb ...\n",
            "Unpacking fonts-liberation2 (2.1.5-1) ...\n",
            "Selecting previously unselected package fonts-linuxlibertine.\n",
            "Preparing to unpack .../059-fonts-linuxlibertine_5.3.0-6_all.deb ...\n",
            "Unpacking fonts-linuxlibertine (5.3.0-6) ...\n",
            "Selecting previously unselected package fonts-noto-core.\n",
            "Preparing to unpack .../060-fonts-noto-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-extra.\n",
            "Preparing to unpack .../061-fonts-noto-extra_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-extra (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../062-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-ui-core.\n",
            "Preparing to unpack .../063-fonts-noto-ui-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-ui-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-sil-gentium.\n",
            "Preparing to unpack .../064-fonts-sil-gentium_20081126%3a1.03-4_all.deb ...\n",
            "Unpacking fonts-sil-gentium (20081126:1.03-4) ...\n",
            "Selecting previously unselected package fonts-sil-gentium-basic.\n",
            "Preparing to unpack .../065-fonts-sil-gentium-basic_1.102-1.1_all.deb ...\n",
            "Unpacking fonts-sil-gentium-basic (1.102-1.1) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../066-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libgstreamer-gl1.0-0:amd64.\n",
            "Preparing to unpack .../067-libgstreamer-gl1.0-0_1.20.1-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking libgstreamer-gl1.0-0:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Selecting previously unselected package gstreamer1.0-gl:amd64.\n",
            "Preparing to unpack .../068-gstreamer1.0-gl_1.20.1-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking gstreamer1.0-gl:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Selecting previously unselected package gstreamer1.0-gtk3:amd64.\n",
            "Preparing to unpack .../069-gstreamer1.0-gtk3_1.20.3-0ubuntu1.3_amd64.deb ...\n",
            "Unpacking gstreamer1.0-gtk3:amd64 (1.20.3-0ubuntu1.3) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../070-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libabw-0.1-1:amd64.\n",
            "Preparing to unpack .../071-libabw-0.1-1_0.1.3-1build3_amd64.deb ...\n",
            "Unpacking libabw-0.1-1:amd64 (0.1.3-1build3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../072-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../073-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../074-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../075-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libbsh-java.\n",
            "Preparing to unpack .../076-libbsh-java_2.0b4-20_all.deb ...\n",
            "Unpacking libbsh-java (2.0b4-20) ...\n",
            "Selecting previously unselected package libcdr-0.1-1:amd64.\n",
            "Preparing to unpack .../077-libcdr-0.1-1_0.1.6-2build2_amd64.deb ...\n",
            "Unpacking libcdr-0.1-1:amd64 (0.1.6-2build2) ...\n",
            "Selecting previously unselected package libsuitesparseconfig5:amd64.\n",
            "Preparing to unpack .../078-libsuitesparseconfig5_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libcolamd2:amd64.\n",
            "Preparing to unpack .../079-libcolamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libe-book-0.1-1:amd64.\n",
            "Preparing to unpack .../080-libe-book-0.1-1_0.1.3-2build2_amd64.deb ...\n",
            "Unpacking libe-book-0.1-1:amd64 (0.1.3-2build2) ...\n",
            "Selecting previously unselected package libel-api-java.\n",
            "Preparing to unpack .../081-libel-api-java_3.0.0-3_all.deb ...\n",
            "Unpacking libel-api-java (3.0.0-3) ...\n",
            "Selecting previously unselected package libepubgen-0.1-1:amd64.\n",
            "Preparing to unpack .../082-libepubgen-0.1-1_0.1.1-1ubuntu5_amd64.deb ...\n",
            "Unpacking libepubgen-0.1-1:amd64 (0.1.1-1ubuntu5) ...\n",
            "Selecting previously unselected package libetonyek-0.1-1:amd64.\n",
            "Preparing to unpack .../083-libetonyek-0.1-1_0.1.10-3build1_amd64.deb ...\n",
            "Unpacking libetonyek-0.1-1:amd64 (0.1.10-3build1) ...\n",
            "Selecting previously unselected package libfreehand-0.1-1.\n",
            "Preparing to unpack .../084-libfreehand-0.1-1_0.1.2-3build2_amd64.deb ...\n",
            "Unpacking libfreehand-0.1-1 (0.1.2-3build2) ...\n",
            "Selecting previously unselected package libservlet-api-java.\n",
            "Preparing to unpack .../085-libservlet-api-java_4.0.1-2_all.deb ...\n",
            "Unpacking libservlet-api-java (4.0.1-2) ...\n",
            "Selecting previously unselected package libjsp-api-java.\n",
            "Preparing to unpack .../086-libjsp-api-java_2.3.4-3_all.deb ...\n",
            "Unpacking libjsp-api-java (2.3.4-3) ...\n",
            "Selecting previously unselected package libwebsocket-api-java.\n",
            "Preparing to unpack .../087-libwebsocket-api-java_1.1-2_all.deb ...\n",
            "Unpacking libwebsocket-api-java (1.1-2) ...\n",
            "Selecting previously unselected package libservlet3.1-java.\n",
            "Preparing to unpack .../088-libservlet3.1-java_1%3a4.0.1-2_all.deb ...\n",
            "Unpacking libservlet3.1-java (1:4.0.1-2) ...\n",
            "Selecting previously unselected package libhsqldb1.8.0-java.\n",
            "Preparing to unpack .../089-libhsqldb1.8.0-java_1.8.0.10+dfsg-11_all.deb ...\n",
            "Unpacking libhsqldb1.8.0-java (1.8.0.10+dfsg-11) ...\n",
            "Selecting previously unselected package libunoloader-java.\n",
            "Preparing to unpack .../090-libunoloader-java_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libunoloader-java (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package ure-java.\n",
            "Preparing to unpack .../091-ure-java_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking ure-java (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package liblibreoffice-java.\n",
            "Preparing to unpack .../092-liblibreoffice-java_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking liblibreoffice-java (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libmspub-0.1-1:amd64.\n",
            "Preparing to unpack .../093-libmspub-0.1-1_0.1.4-3build3_amd64.deb ...\n",
            "Unpacking libmspub-0.1-1:amd64 (0.1.4-3build3) ...\n",
            "Selecting previously unselected package libmwaw-0.3-3:amd64.\n",
            "Preparing to unpack .../094-libmwaw-0.3-3_0.3.21-1build1_amd64.deb ...\n",
            "Unpacking libmwaw-0.3-3:amd64 (0.3.21-1build1) ...\n",
            "Selecting previously unselected package libodfgen-0.1-1:amd64.\n",
            "Preparing to unpack .../095-libodfgen-0.1-1_0.1.8-2build2_amd64.deb ...\n",
            "Unpacking libodfgen-0.1-1:amd64 (0.1.8-2build2) ...\n",
            "Selecting previously unselected package libpagemaker-0.0-0:amd64.\n",
            "Preparing to unpack .../096-libpagemaker-0.0-0_0.0.4-1build3_amd64.deb ...\n",
            "Unpacking libpagemaker-0.0-0:amd64 (0.0.4-1build3) ...\n",
            "Selecting previously unselected package lp-solve.\n",
            "Preparing to unpack .../097-lp-solve_5.5.2.5-2build2_amd64.deb ...\n",
            "Unpacking lp-solve (5.5.2.5-2build2) ...\n",
            "Selecting previously unselected package libwps-0.4-4:amd64.\n",
            "Preparing to unpack .../098-libwps-0.4-4_0.4.12-2build1_amd64.deb ...\n",
            "Unpacking libwps-0.4-4:amd64 (0.4.12-2build1) ...\n",
            "Selecting previously unselected package libreoffice-calc.\n",
            "Preparing to unpack .../099-libreoffice-calc_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-calc (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libvisio-0.1-1:amd64.\n",
            "Preparing to unpack .../100-libvisio-0.1-1_0.1.7-1build5_amd64.deb ...\n",
            "Unpacking libvisio-0.1-1:amd64 (0.1.7-1build5) ...\n",
            "Selecting previously unselected package libwpd-0.10-10:amd64.\n",
            "Preparing to unpack .../101-libwpd-0.10-10_0.10.3-2build1_amd64.deb ...\n",
            "Unpacking libwpd-0.10-10:amd64 (0.10.3-2build1) ...\n",
            "Selecting previously unselected package libwpg-0.3-3:amd64.\n",
            "Preparing to unpack .../102-libwpg-0.3-3_0.3.3-1build3_amd64.deb ...\n",
            "Unpacking libwpg-0.3-3:amd64 (0.3.3-1build3) ...\n",
            "Selecting previously unselected package libreoffice-draw.\n",
            "Preparing to unpack .../103-libreoffice-draw_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-draw (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-impress.\n",
            "Preparing to unpack .../104-libreoffice-impress_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-impress (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-math.\n",
            "Preparing to unpack .../105-libreoffice-math_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-math (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-report-builder-bin.\n",
            "Preparing to unpack .../106-libreoffice-report-builder-bin_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-report-builder-bin (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-writer.\n",
            "Preparing to unpack .../107-libreoffice-writer_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-writer (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package python3-uno.\n",
            "Preparing to unpack .../108-python3-uno_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking python3-uno (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice.\n",
            "Preparing to unpack .../109-libreoffice_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-gnome.\n",
            "Preparing to unpack .../110-libreoffice-gnome_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-gnome (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-gtk3.\n",
            "Preparing to unpack .../111-libreoffice-gtk3_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-gtk3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-java-common.\n",
            "Preparing to unpack .../112-libreoffice-java-common_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-java-common (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-report-builder.\n",
            "Preparing to unpack .../113-libreoffice-report-builder_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-report-builder (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-script-provider-bsh.\n",
            "Preparing to unpack .../114-libreoffice-script-provider-bsh_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-script-provider-bsh (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-script-provider-js.\n",
            "Preparing to unpack .../115-libreoffice-script-provider-js_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-script-provider-js (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-script-provider-python.\n",
            "Preparing to unpack .../116-libreoffice-script-provider-python_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-script-provider-python (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-firebird.\n",
            "Preparing to unpack .../117-libreoffice-sdbc-firebird_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-firebird (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-hsqldb.\n",
            "Preparing to unpack .../118-libreoffice-sdbc-hsqldb_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-hsqldb (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-mysql.\n",
            "Preparing to unpack .../119-libreoffice-sdbc-mysql_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-mysql (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-postgresql.\n",
            "Preparing to unpack .../120-libreoffice-sdbc-postgresql_1%3a7.3.7-0ubuntu0.22.04.10_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-postgresql (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-style-elementary.\n",
            "Preparing to unpack .../121-libreoffice-style-elementary_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-style-elementary (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-style-yaru.\n",
            "Preparing to unpack .../122-libreoffice-style-yaru_1%3a7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-style-yaru (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package libreoffice-wiki-publisher.\n",
            "Preparing to unpack .../123-libreoffice-wiki-publisher_1.2.0+LibO7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-wiki-publisher (1.2.0+LibO7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package poppler-utils.\n",
            "Preparing to unpack .../124-poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Selecting previously unselected package libreoffice-nlpsolver.\n",
            "Preparing to unpack .../125-libreoffice-nlpsolver_0.9+LibO7.3.7-0ubuntu0.22.04.10_all.deb ...\n",
            "Unpacking libreoffice-nlpsolver (0.9+LibO7.3.7-0ubuntu0.22.04.10) ...\n",
            "Selecting previously unselected package unoconv.\n",
            "Preparing to unpack .../126-unoconv_0.7-2ubuntu1_all.deb ...\n",
            "Unpacking unoconv (0.7-2ubuntu1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up fonts-sil-gentium-basic (1.102-1.1) ...\n",
            "Setting up libharfbuzz-icu0:amd64 (2.7.4-1ubuntu3.2) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libtommath1:amd64 (1.2.0-6ubuntu0.22.04.1) ...\n",
            "Setting up fonts-noto-extra (20201225-1build1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libyajl2:amd64 (2.1.0-3ubuntu0.22.04.1) ...\n",
            "Setting up libuno-sal3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libel-api-java (3.0.0-3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libeot0:amd64 (0.01-5build2) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libgpgme11:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Setting up firebird3.0-common-doc (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up librevenge-0.0-0:amd64 (0.0.4-6ubuntu7) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up fonts-crosextra-carlito (20130920-1.1) ...\n",
            "Setting up firebird3.0-common (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service â†’ /lib/systemd/system/apparmor.service.\n",
            "Setting up fonts-sil-gentium (20081126:1.03-4) ...\n",
            "Setting up libreoffice-style-colibre (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up liborcus-parser-0.17-0:amd64 (0.17.2-2) ...\n",
            "Setting up fonts-liberation2 (2.1.5-1) ...\n",
            "Setting up libwebsocket-api-java (1.1-2) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libfreehand-0.1-1 (0.1.2-3build2) ...\n",
            "Setting up libclucene-core1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Setting up libabsl20210324:amd64 (0~20210324.2-2ubuntu0.2) ...\n",
            "Setting up fonts-linuxlibertine (5.3.0-6) ...\n",
            "Setting up libbsh-java (2.0b4-20) ...\n",
            "Setting up libjsp-api-java (2.3.4-3) ...\n",
            "Setting up libmhash2:amd64 (0.9.9.9-9build2) ...\n",
            "Setting up libmythes-1.2-0:amd64 (2:1.2.4-4build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libexttextcat-data (3.4.5-1build2) ...\n",
            "Setting up libabw-0.1-1:amd64 (0.1.3-1build3) ...\n",
            "Setting up libservlet-api-java (4.0.1-2) ...\n",
            "Setting up libepubgen-0.1-1:amd64 (0.1.1-1ubuntu5) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libuno-salhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up fonts-crosextra-caladea (20130214-2.1) ...\n",
            "Setting up libreoffice-style-yaru (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up liblangtag-common (0.6.3-2ubuntu1) ...\n",
            "Setting up libib-util:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libunoloader-java (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up fonts-noto-ui-core (20201225-1build1) ...\n",
            "Setting up libxmlsec1:amd64 (1.2.33-1build2) ...\n",
            "Setting up libwpd-0.10-10:amd64 (0.10.3-2build1) ...\n",
            "Setting up fonts-noto-core (20201225-1build1) ...\n",
            "Setting up libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up fonts-opensymbol (2:102.12+LibO7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libservlet3.1-java (1:4.0.1-2) ...\n",
            "Setting up libodfgen-0.1-1:amd64 (0.1.8-2build2) ...\n",
            "Setting up libvisio-0.1-1:amd64 (0.1.7-1build5) ...\n",
            "Setting up libreoffice-style-elementary (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up fonts-dejavu (2.37-2build1) ...\n",
            "Setting up libwps-0.4-4:amd64 (0.4.12-2build1) ...\n",
            "Setting up libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libexttextcat-2.0-0:amd64 (3.4.5-1build2) ...\n",
            "Setting up libgpgmepp6:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Setting up libmspub-0.1-1:amd64 (0.1.4-3build3) ...\n",
            "Setting up libraptor2-0:amd64 (2.0.15-0ubuntu4.1) ...\n",
            "Setting up lp-solve (5.5.2.5-2build2) ...\n",
            "Setting up libpagemaker-0.0-0:amd64 (0.0.4-1build3) ...\n",
            "Setting up libmwaw-0.3-3:amd64 (0.3.21-1build1) ...\n",
            "Setting up libcdr-0.1-1:amd64 (0.1.6-2build2) ...\n",
            "Setting up liblangtag1:amd64 (0.6.3-2ubuntu1) ...\n",
            "Setting up libfbclient2:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up liborcus-0.17-0:amd64 (0.17.2-2) ...\n",
            "Setting up libgstreamer-gl1.0-0:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Setting up firebird3.0-utils (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up libuno-cppu3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libhsqldb1.8.0-java (1.8.0.10+dfsg-11) ...\n",
            "Setting up libclucene-contribs1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Setting up libwpg-0.3-3:amd64 (0.3.3-1build3) ...\n",
            "Setting up libxmlsec1-nss:amd64 (1.2.33-1build2) ...\n",
            "Setting up gstreamer1.0-gl:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Setting up libuno-purpenvhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up uno-libs-private (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up firebird3.0-server-core:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up librasqal3:amd64 (0.9.33-0.2ubuntu1) ...\n",
            "Setting up libetonyek-0.1-1:amd64 (0.1.10-3build1) ...\n",
            "Setting up gstreamer1.0-gtk3:amd64 (1.20.3-0ubuntu1.3) ...\n",
            "Setting up libe-book-0.1-1:amd64 (0.1.3-2build2) ...\n",
            "Setting up librdf0:amd64 (1.0.17-1.1ubuntu3) ...\n",
            "Setting up libuno-cppuhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up ure (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-common (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/main.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/pdfimport.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/xsltfilter.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/lingucomponent.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/Langpack-en-US.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/res/fcfg_langpack_en-US.xcd with new version\n",
            "Setting up ure-java (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-core (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-math (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/math.xcd with new version\n",
            "Setting up libreoffice-gtk3 (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-sdbc-postgresql (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/postgresql.xcd with new version\n",
            "Setting up liblibreoffice-java (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-draw (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/draw.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/graphicfilter.xcd with new version\n",
            "Setting up libreoffice-java-common (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-base-drivers (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-wiki-publisher (1.2.0+LibO7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-sdbc-firebird (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-sdbc-mysql (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-gnome (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/gnome.xcd with new version\n",
            "Setting up libreoffice-impress (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/impress.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/ogltrans.xcd with new version\n",
            "Setting up libreoffice-base-core (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up python3-uno (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/pyuno.xcd with new version\n",
            "Setting up libreoffice-script-provider-bsh (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-script-provider-js (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-calc (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/calc.xcd with new version\n",
            "Setting up libreoffice-base (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/base.xcd with new version\n",
            "Setting up libreoffice-sdbc-hsqldb (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-writer (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/writer.xcd with new version\n",
            "Setting up libreoffice-script-provider-python (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-nlpsolver (0.9+LibO7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-report-builder-bin (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up unoconv (0.7-2ubuntu1) ...\n",
            "Setting up libreoffice (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "Setting up libreoffice-report-builder (1:7.3.7-0ubuntu0.22.04.10) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/reportbuilder.xcd with new version\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for shared-mime-info (2.1-2) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "--- Dependencies installed/updated ---\n"
          ]
        }
      ],
      "source": [
        "# --- 0. Installations ---\n",
        "!pip install Flask Flask-CORS pyngrok -q\n",
        "!pip install --upgrade pymupdf Flask Flask-CORS pyngrok loguru -q\n",
        "!pip install langchain tiktoken sentence-transformers accelerate chromadb torch numpy pdfplumber pdf2image pytesseract opencv-python-headless Pillow huggingface_hub python-dotenv -q\n",
        "!pip install azure-ai-inference openai langchain-openai -q\n",
        "!pip install python-docx openpyxl python-pptx -q\n",
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y tesseract-ocr tesseract-ocr-eng poppler-utils libreoffice unoconv -qq\n",
        "print(\"--- Dependencies installed/updated ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl5FhpyDcIfh",
        "outputId": "6e2329d2-6894-4f53-81b4-2a31712756bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "AZURE_EMBEDDING_API_KEY_SECRET = 'KEY' # add your own key\n",
        "HF_API_TOKEN_SECRET = 'KEY' # add your own key\n",
        "!ngrok config add-authtoken KEY # add your own key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBmZan3BcQRv",
        "outputId": "2c3f586a-6755-4124-900e-e07a6b9cab32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:16:18.063 | INFO     | __main__:<cell line: 0>:62 - AskYourDoc Merged Flask App Starting. Log file: ask_your_doc_flask_merged_app.log\n",
            "2025-06-01 09:16:18.064 | INFO     | __main__:<cell line: 0>:68 - HF_TOKEN loaded.\n",
            "2025-06-01 09:16:18.064 | INFO     | __main__:<cell line: 0>:75 - Azure Embedding Model: text-embedding-3-large configured.\n",
            "2025-06-01 09:16:18.064 | WARNING  | __main__:<cell line: 0>:76 - Using a GitHub token for Azure AI Inference API Key. Please verify this is the correct type of key for your Azure endpoint.\n",
            "2025-06-01 09:16:18.064 | INFO     | __main__:<cell line: 0>:83 - Azure LLM Model: gpt-4o configured.\n",
            "2025-06-01 09:16:18.079 | INFO     | __main__:perform_initial_setup:1598 - Flask App: Performing one-time initial setup for AI/DB systems...\n",
            "2025-06-01 09:16:18.079 | WARNING  | __main__:initialize_rag_models_from_kb:244 - Tiktoken fail: name 'tiktoken' is not defined. Using len().\n",
            "2025-06-01 09:16:18.080 | INFO     | __main__:initialize_rag_models_from_kb:253 - Loading Azure Embeddings client: text-embedding-3-large...\n",
            "2025-06-01 09:16:18.081 | SUCCESS  | __main__:initialize_rag_models_from_kb:259 - Azure Embeddings client for 'text-embedding-3-large' initialized.\n",
            "2025-06-01 09:16:18.081 | INFO     | __main__:initialize_rag_models_from_kb:266 - Loading reranker: cross-encoder/ms-marco-MiniLM-L-12-v2 (cpu)...\n",
            "2025-06-01 09:16:18.081 | WARNING  | __main__:initialize_rag_models_from_kb:275 - Reranker load FAIL: name 'CrossEncoder' is not defined. Reranking off.\n",
            "2025-06-01 09:16:18.373 | SUCCESS  | __main__:initialize_llm_model:218 - OpenAI client for 'gpt-4o' initialized.\n",
            "2025-06-01 09:16:19.007 | SUCCESS  | __main__:initialize_llm_model:227 - LangChain LLM wrapper for 'gpt-4o' initialized for potential summarization.\n",
            "2025-06-01 09:16:19.008 | INFO     | __main__:initialize_chromadb_from_kb:286 - Initializing ChromaDB: path='./ask_your_doc_chroma_db_merged_flask'\n",
            "2025-06-01 09:16:19.326 | SUCCESS  | __main__:initialize_chromadb_from_kb:289 - ChromaDB persistent OK: './ask_your_doc_chroma_db_merged_flask'.\n",
            "2025-06-01 09:16:19.327 | SUCCESS  | __main__:perform_initial_setup:1614 - Flask App: All critical AI/DB systems initialized.\n",
            "2025-06-01 09:16:20.348 | WARNING  | __main__:start_flask_app_cell4_final:1922 - NGROK_AUTHTOKEN secret not found. Proceeding without ngrok authentication.\n",
            "2025-06-01 09:16:20.780 | INFO     | __main__:start_flask_app_cell4_final:1933 - Flask application (Merged FINAL Version - NoCite) is accessible at: NgrokTunnel: \"https://2346-35-222-69-135.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Ngrok tunnel (FINAL NoCite Version) \"NgrokTunnel: \"https://2346-35-222-69-135.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:18:24] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:18:26] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:18:52.833 | INFO     | __main__:stage_single_file_route:1655 - --- API Call: /stage_file ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:18:53] \"POST /stage_file HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:18:53.223 | INFO     | __main__:stage_single_file_route:1679 -   Staged 'HastiVadariyaResume (1) (1).pdf' (ID:staged_81a5e222_HastiVadariyaResume__1___1_.pdf, Size:114517B) to '/content/colab_staged_uploads_final_nocite/staged_81a5e222_HastiVadariyaResume__1___1_.pdf'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:19:02.792 | INFO     | __main__:process_staged_files_route:1692 - --- API Call: /process_staged_files ---\n",
            "2025-06-01 09:19:02.794 | INFO     | __main__:process_staged_files_route:1704 -   Processing options: {'textOnly': True, 'ocr': False, 'handwritten': False, 'images': False}. Staging IDs: ['staged_81a5e222_HastiVadariyaResume__1___1_.pdf']\n",
            "2025-06-01 09:19:02.795 | INFO     | __main__:process_staged_files_route:1719 -   Processing 'HastiVadariyaResume (1) (1).pdf' (ID: staged_81a5e222_HastiVadariyaResume__1___1_.pdf) from '/content/colab_staged_uploads_final_nocite/staged_81a5e222_HastiVadariyaResume__1___1_.pdf'\n",
            "2025-06-01 09:19:02.795 | INFO     | __main__:process_staged_files_route:1729 -     Effective parsing profile for 'HastiVadariyaResume (1) (1).pdf': 'fastest'\n",
            "2025-06-01 09:19:02.795 | INFO     | __main__:parse_document_via_profile:844 - Initiating parsing process for document 'staged_81a5e222_HastiVadariyaResume__1___1_.pdf' with profile: 'fastest'\n",
            "2025-06-01 09:19:02.795 | INFO     | __main__:parse_document_via_profile:872 - Final config for parsing 'staged_81a5e222_HastiVadariyaResume__1___1_.pdf' (profile 'fastest'): {'use_pdfplumber_tables': True, 'use_pymupdf_text': True, 'process_scanned_pages': False, 'process_structured_images': False, 'use_vision_for_ocr': False, 'use_vision_for_description': False, 'scan_detection_char_threshold': 120, 'dpi_for_conversion': 220}\n",
            "2025-06-01 09:19:02.795 | INFO     | __main__:convert_to_pdf:799 -   File 'staged_81a5e222_HastiVadariyaResume__1___1_.pdf' is already a PDF. Skipping conversion.\n",
            "2025-06-01 09:19:02.795 | INFO     | __main__:parse_pdf_content_worker:448 -   Worker: Parsing PDF content from 'staged_81a5e222_HastiVadariyaResume__1___1_.pdf' (originally pdf) with config: {'use_pdfplumber_tables': True, 'use_pymupdf_text': True, 'process_scanned_pages': False, 'process_structured_images': False, 'use_vision_for_ocr': False, 'use_vision_for_description': False, 'scan_detection_char_threshold': 120, 'dpi_for_conversion': 220}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:19:03.648 | INFO     | __main__:parse_pdf_content_worker:533 -   Doc stats: Pages:1, AvgFont:9.6, StdFont:6.0\n",
            "2025-06-01 09:19:03.649 | WARNING  | __main__:parse_pdf_content_worker:563 -     P1: Older PyMuPDF version, using basic text flags.\n",
            "2025-06-01 09:19:03.685 | INFO     | __main__:parse_pdf_content_worker:765 -   Worker: Finished parsing 'staged_81a5e222_HastiVadariyaResume__1___1_.pdf' (originally pdf). Total blocks extracted: 32\n",
            "2025-06-01 09:19:03.686 | SUCCESS  | __main__:parse_document_via_profile:893 - Profile 'fastest' parsing for 'staged_81a5e222_HastiVadariyaResume__1___1_.pdf' (converted from pdf) in 0.89s. Blocks: 32.\n",
            "2025-06-01 09:19:03.715 | INFO     | __main__:get_or_create_collection_cached:317 - Collection 'askdocmerged_hastivadariyaresume_1_1__80d0ed' accessed/created. Items: 0\n",
            "2025-06-01 09:19:03.715 | INFO     | __main__:ingest_document_into_knowledge_base:1129 - Ingesting 'HastiVadariyaResume (1) (1).pdf' into collection: askdocmerged_hastivadariyaresume_1_1__80d0ed\n",
            "2025-06-01 09:19:03.716 | INFO     | __main__:_chunk_parsed_blocks:1049 - Chunking for 'HastiVadariyaResume (1) (1).pdf': 32 final chunks.\n",
            "2025-06-01 09:19:03.716 | INFO     | __main__:_embed_chunks:1062 - Embedding 32 chunks using Azure OpenAI 'text-embedding-3-large'...\n",
            "2025-06-01 09:19:04.927 | SUCCESS  | __main__:_embed_chunks:1076 - Embedded 32 chunks using Azure OpenAI.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:19:05] \"POST /process_staged_files HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:19:05.633 | SUCCESS  | __main__:_store_chunks_in_chromadb:1118 - Stored 32 chunks. Collection 'askdocmerged_hastivadariyaresume_1_1__80d0ed' total: 32.\n",
            "2025-06-01 09:19:05.633 | SUCCESS  | __main__:ingest_document_into_knowledge_base:1151 - Ingest 'HastiVadariyaResume (1) (1).pdf' (askdocmerged_hastivadariyaresume_1_1__80d0ed) in 1.92s. Stored: 32.\n",
            "2025-06-01 09:19:05.635 | INFO     | __main__:process_staged_files_route:1765 -     Successfully ingested 'HastiVadariyaResume (1) (1).pdf' into collection 'askdocmerged_hastivadariyaresume_1_1__80d0ed'.\n",
            "2025-06-01 09:19:05.636 | INFO     | __main__:process_staged_files_route:1768 -     Removed staged file '/content/colab_staged_uploads_final_nocite/staged_81a5e222_HastiVadariyaResume__1___1_.pdf'.\n",
            "2025-06-01 09:19:05.636 | INFO     | __main__:process_staged_files_route:1785 - --- /process_staged_files call ended. Successfully processed: 1, Errors: 0 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:19:06] \"GET /chatui HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:19:07] \"GET /static/js/documents.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:19:07] \"GET /static/js/chat.js HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:19:15.558 | INFO     | __main__:ask_question_route:1799 - --- API Call: /ask ---\n",
            "2025-06-01 09:19:15.559 | INFO     | __main__:ask_question_route:1810 -   Received query for /ask: 'hey whats the document abot...'\n",
            "2025-06-01 09:19:15.559 | INFO     | __main__:ask_question_route:1817 -   Frontend specified active documents: ['HastiVadariyaResume (1) (1).pdf']\n",
            "2025-06-01 09:19:15.559 | INFO     | __main__:ask_question_route:1835 -   Query will target specific collections: ['askdocmerged_hastivadariyaresume_1_1__80d0ed']\n",
            "2025-06-01 09:19:15.896 | SUCCESS  | __main__:retrieve_relevant_context:1355 - Context retrieval for 'hey whats the document abot...' in 0.34s. Final chunks: 12.\n",
            "2025-06-01 09:19:15.897 | INFO     | __main__:generate_standard_llm_response:1410 - Sending RAG request to OpenAI ('gpt-4o'). Query: 'hey whats the document abot...'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:19:19] \"POST /ask HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:19:19.272 | SUCCESS  | __main__:generate_standard_llm_response:1426 - OpenAI LLM response in 3.37s.\n",
            "2025-06-01 09:19:19.273 | INFO     | __main__:ask_question_route:1885 - --- /ask call finished. Answer length: 428. Summarization used: False ---\n",
            "2025-06-01 09:19:45.482 | INFO     | __main__:ask_question_route:1799 - --- API Call: /ask ---\n",
            "2025-06-01 09:19:45.484 | INFO     | __main__:ask_question_route:1810 -   Received query for /ask: 'can you tell me summary of her current job...'\n",
            "2025-06-01 09:19:45.484 | INFO     | __main__:ask_question_route:1817 -   Frontend specified active documents: ['HastiVadariyaResume (1) (1).pdf']\n",
            "2025-06-01 09:19:45.484 | INFO     | __main__:ask_question_route:1835 -   Query will target specific collections: ['askdocmerged_hastivadariyaresume_1_1__80d0ed']\n",
            "2025-06-01 09:19:45.645 | SUCCESS  | __main__:retrieve_relevant_context:1355 - Context retrieval for 'can you tell me summary of her...' in 0.16s. Final chunks: 12.\n",
            "2025-06-01 09:19:45.646 | INFO     | __main__:generate_standard_llm_response:1410 - Sending RAG request to OpenAI ('gpt-4o'). Query: 'can you tell me summary of her current job...'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jun/2025 09:19:50] \"POST /ask HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 09:19:50.025 | SUCCESS  | __main__:generate_standard_llm_response:1426 - OpenAI LLM response in 4.38s.\n",
            "2025-06-01 09:19:50.026 | INFO     | __main__:ask_question_route:1885 - --- /ask call finished. Answer length: 1374. Summarization used: False ---\n"
          ]
        }
      ],
      "source": [
        "# Backend\n",
        "\n",
        "# --- 1. Imports ---\n",
        "import base64\n",
        "import io\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "import uuid\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import chromadb\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import torch\n",
        "from flask import Flask, jsonify, render_template, request, send_from_directory\n",
        "from flask_cors import CORS\n",
        "from google.colab import files as colab_files # Specific to Colab, handle if not in Colab\n",
        "from google.colab import userdata # Specific to Colab\n",
        "from huggingface_hub import InferenceClient\n",
        "from huggingface_hub.utils import (GatedRepoError, HFValidationError,\n",
        "                                   RepositoryNotFoundError)\n",
        "from langchain.chains.summarize import \\\n",
        "    load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from loguru import logger\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "# New Azure Imports\n",
        "from azure.ai.inference import EmbeddingsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "# New OpenAI Imports for LLM\n",
        "from openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI # For LangChain integration with OpenAI\n",
        "\n",
        "import cv2\n",
        "\n",
        "from docx import Document as DocxDocument\n",
        "from openpyxl import load_workbook\n",
        "# Note: python-pptx doesn't have a direct save-to-PDF method, unoconv is best here.\n",
        "# from pptx import Presentation as PptxPresentation\n",
        "# from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
        "# from pptx.util import Inches\n",
        "\n",
        "# --- 2. Loguru Configuration ---\n",
        "logger.remove()\n",
        "logger.add(\n",
        "    lambda msg: print(msg, end=\"\"),\n",
        "    level=\"INFO\",\n",
        "    format=\"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\",\n",
        ")\n",
        "LOG_FILE_PATH = \"ask_your_doc_flask_merged_app.log\"\n",
        "logger.add(LOG_FILE_PATH, rotation=\"10 MB\", level=\"DEBUG\")\n",
        "logger.info(f\"AskYourDoc Merged Flask App Starting. Log file: {LOG_FILE_PATH}\")\n",
        "\n",
        "# --- 3. Configuration (Combined and Prioritized) ---\n",
        "HF_API_TOKEN_SECRET = None\n",
        "\n",
        "try:\n",
        "    logger.info(\"HF_TOKEN loaded.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    logger.warning(\"HF_TOKEN not in Colab secrets.\")\n",
        "\n",
        "# AZURE EMBEDDING CONFIGURATION\n",
        "AZURE_EMBEDDING_ENDPOINT = \"https://models.inference.ai.azure.com\"\n",
        "AZURE_EMBEDDING_MODEL_NAME = \"text-embedding-3-large\"\n",
        "logger.info(f\"Azure Embedding Model: {AZURE_EMBEDDING_MODEL_NAME} configured.\")\n",
        "logger.warning(\"Using a GitHub token for Azure AI Inference API Key. Please verify this is the correct type of key for your Azure endpoint.\")\n",
        "\n",
        "\n",
        "# AZURE LLM CONFIGURATION (using the same endpoint and key as embedding)\n",
        "AZURE_LLM_ENDPOINT = AZURE_EMBEDDING_ENDPOINT # Reusing the endpoint\n",
        "AZURE_LLM_MODEL_NAME = \"gpt-4o\"\n",
        "AZURE_LLM_API_KEY_SECRET = AZURE_EMBEDDING_API_KEY_SECRET # Reusing the token for both\n",
        "logger.info(f\"Azure LLM Model: {AZURE_LLM_MODEL_NAME} configured.\")\n",
        "\n",
        "\n",
        "# From Parsing Script\n",
        "VISION_MODEL_ID_CONFIG = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
        "IMAGE_OCR_PROMPT_CONFIG = \"You are an advanced OCR model. Transcribe all text from this image accurately. If it's handwritten, do your best to read it. If there is no text, output 'No text found'.\"\n",
        "IMAGE_DESCRIPTION_PROMPT_CONFIG = \"You are an expert image analyst. Describe this image in detail, focusing on elements relevant to understanding a document (e.g., charts, diagrams, important visual features). If the image primarily contains text, transcribe the text instead of describing it as a visual.\"\n",
        "\n",
        "\n",
        "# From KB/Retrieval Script\n",
        "EMBEDDING_MODEL_NAME = AZURE_EMBEDDING_MODEL_NAME # Reflect the new model for logging\n",
        "EMBEDDING_MODEL_MAX_TOKENS = 8192 # Max input tokens for text-embedding-3-large\n",
        "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
        "RERANKER_MAX_TOKENS = 512 # Max sequence length for reranker.\n",
        "\n",
        "# Adjusted for larger context window of text-embedding-3-large (8192 tokens)\n",
        "PRIMARY_CHUNK_SIZE_TOKENS = 1500\n",
        "PRIMARY_CHUNK_OVERLAP_TOKENS = 150\n",
        "MIN_CHUNK_SIZE_TOKENS_NO_SPLIT = 75\n",
        "EFFECTIVE_MAX_CHUNK_TOKENS = EMBEDDING_MODEL_MAX_TOKENS - 5\n",
        "ATOMIC_BLOCK_NO_SPLIT_THRESHOLD = EFFECTIVE_MAX_CHUNK_TOKENS\n",
        "\n",
        "SEPARATORS_HIERARCHICAL = [\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \", \", \" \", \"\"]\n",
        "\n",
        "# From Generation Script\n",
        "GENERATION_MODEL_NAME = AZURE_LLM_MODEL_NAME # Reflect the new LLM model for logging\n",
        "\n",
        "CHROMA_DB_PATH = \"./ask_your_doc_chroma_db_merged_flask\"\n",
        "CHROMA_COLLECTION_NAME_PREFIX = \"askdocmerged\"\n",
        "MAX_TOKENS_FOR_DIRECT_LLM_SUMMARIZATION = 80000\n",
        "\n",
        "# --- 4. Global Application State (Unified) ---\n",
        "APP_STATE = {\n",
        "    \"hf_api_token\": HF_API_TOKEN_SECRET,\n",
        "    \"hf_vision_client\": None,\n",
        "    \"vision_client_initialized\": False,\n",
        "    \"langchain_llm\": None,\n",
        "    \"llm_initialized\": False,\n",
        "    \"azure_llm_client\": None,\n",
        "    \"tokenizer\": None,\n",
        "    \"embedding_model\": None,\n",
        "    \"azure_embedding_client\": None,\n",
        "    \"reranker_model\": None,\n",
        "    \"rag_models_loaded\": False,\n",
        "    \"chroma_client\": None,\n",
        "    \"db_initialized\": False,\n",
        "    \"chroma_collection_cache\": {},\n",
        "    \"original_doc_filename_for_chunking_context\": None,\n",
        "}\n",
        "# Web-specific state\n",
        "APP_WEB_STATE = {\n",
        "    \"staged_files\": {},\n",
        "    \"processed_collections\": {},\n",
        "}\n",
        "\n",
        "# --- 5. AI Persona and System Instructions (No Inline Citations) ---\n",
        "AI_PERSONA_NAME = \"DocuMind AI\"\n",
        "AI_ROLE_DESCRIPTION = (\n",
        "    \"a meticulous and highly accurate AI assistant, designed to interact with \"\n",
        "    \"and answer questions about documents. You are an expert in information \"\n",
        "    \"retrieval and text comprehension.\"\n",
        ")\n",
        "AI_CORE_DIRECTIVE = (\n",
        "    \"Your responses must be grounded *exclusively* in the provided context data. \"\n",
        "    \"You do not have access to external knowledge or the internet. \"\n",
        "    \"You must not invent, assume, or infer information beyond what is explicitly stated in the context. \"\n",
        "    \"Accuracy and adherence to the provided context are paramount. \"\n",
        "    \"DO NOT invent data columns, values, or statuses that are not explicitly written in the PROVIDED CONTEXT.\"\n",
        ")\n",
        "AI_ANSWERING_STYLE_REFINED = (\n",
        "    \"Provide answers that are factual, precise, and directly supported by the text in \"\n",
        "    \"the 'PROVIDED CONTEXT'. Use clear and concise language. If the query asks for \"\n",
        "    \"specific data points (e.g., numbers, dates, names), ensure they are extracted \"\n",
        "    \"accurately. If the query is more general, synthesize the relevant information \"\n",
        "    \"from the context into a coherent response. Avoid jargon unless it's part of the \"\n",
        "    \"document's language and relevant to the answer. You are polite, professional, and helpful.\"\n",
        ")\n",
        "AI_CONTEXTUAL_PRIORITIZATION_POLICY = (\n",
        "    \"When multiple context sources are provided, synthesize information if they are \"\n",
        "    \"complementary. If they are contradictory, point out the discrepancy if relevant \"\n",
        "    \"to the query, or prioritize the most specific or seemingly authoritative source if \"\n",
        "    \"a single answer is required and discernable. If context chunks seem to be out of \"\n",
        "    \"order, try to make sense of them logically if possible, but do not assume continuity \"\n",
        "    \"if it's not evident.\"\n",
        ")\n",
        "AI_CITATION_POLICY_TEXT = (\n",
        "    \"DO NOT include bracketed citations like [Doc: ..., P:..., CkID:...] in your response. \"\n",
        "    \"You may refer to 'the document' or specific document names if it's natural for the answer, \"\n",
        "    \"but avoid detailed source tagging in the answer text.\"\n",
        ")\n",
        "AI_NO_ANSWER_POLICY = (\n",
        "    \"If the context is insufficient to answer the query, you must state: 'Based on the provided document context, I could not find the information to answer this question.'\"\n",
        ")\n",
        "AI_SUMMARIZATION_POLICY = (\n",
        "    \"If the query asks for a summary, provide a concise yet comprehensive overview of the main points \"\n",
        "    \"from the 'PROVIDED CONTEXT'. The summary should be neutral, \"\n",
        "    \"objective, and reflect the key information accurately. Follow the citation policy \"\n",
        "    \"(i.e., no bracketed citations). Focus on the core aspects and main themes.\"\n",
        ")\n",
        "\n",
        "# --- 6. Initialization Functions (Combined and Refined) ---\n",
        "def initialize_hf_client_from_parser():\n",
        "    global APP_STATE\n",
        "    if APP_STATE[\"vision_client_initialized\"]:\n",
        "        return True\n",
        "    if not APP_STATE[\"hf_api_token\"]:\n",
        "        logger.warning(\"HF_TOKEN not found. Vision features disabled.\")\n",
        "        return False\n",
        "    if VISION_MODEL_ID_CONFIG == \"YOUR_NOVITA_SUPPORTED_VISION_MODEL_ID_PLACEHOLDER\":\n",
        "        logger.error(f\"VISION_MODEL_ID_CONFIG is placeholder. Vision disabled.\")\n",
        "        return False\n",
        "    try:\n",
        "        APP_STATE[\"hf_vision_client\"] = InferenceClient(\n",
        "            provider=\"novita\", api_key=HF_API_TOKEN_SECRET\n",
        "        )\n",
        "        logger.success(f\"HF client (Novita) for {VISION_MODEL_ID_CONFIG} initialized.\")\n",
        "        APP_STATE[\"vision_client_initialized\"] = True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"HF client init error: {e}\", exc_info=True)\n",
        "        APP_STATE[\"hf_vision_client\"] = None\n",
        "        APP_STATE[\"vision_client_initialized\"] = False\n",
        "    return APP_STATE[\"vision_client_initialized\"]\n",
        "\n",
        "def initialize_llm_model():\n",
        "    global APP_STATE\n",
        "    if APP_STATE[\"llm_initialized\"]:\n",
        "        return True\n",
        "    if not AZURE_LLM_API_KEY_SECRET:\n",
        "        logger.critical(\"AZURE_LLM_API_KEY_SECRET missing!\")\n",
        "        return False\n",
        "    try:\n",
        "        APP_STATE[\"azure_llm_client\"] = OpenAI(\n",
        "            base_url=AZURE_LLM_ENDPOINT,\n",
        "            api_key=AZURE_LLM_API_KEY_SECRET,\n",
        "        )\n",
        "        logger.success(f\"OpenAI client for '{AZURE_LLM_MODEL_NAME}' initialized.\")\n",
        "\n",
        "        APP_STATE[\"langchain_llm\"] = ChatOpenAI(\n",
        "            model=AZURE_LLM_MODEL_NAME,\n",
        "            openai_api_base=AZURE_LLM_ENDPOINT,\n",
        "            openai_api_key=AZURE_LLM_API_KEY_SECRET,\n",
        "            temperature=0.25,\n",
        "            max_tokens=1000,\n",
        "        )\n",
        "        logger.success(\n",
        "            f\"LangChain LLM wrapper for '{AZURE_LLM_MODEL_NAME}' initialized for potential summarization.\"\n",
        "        )\n",
        "        APP_STATE[\"llm_initialized\"] = True\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"OpenAI LLM init FAILED: {e}\", exc_info=True)\n",
        "        APP_STATE[\"llm_initialized\"] = False\n",
        "    return APP_STATE[\"llm_initialized\"]\n",
        "\n",
        "def initialize_rag_models_from_kb():\n",
        "    global APP_STATE\n",
        "    if APP_STATE[\"rag_models_loaded\"]:\n",
        "        return True\n",
        "    try:\n",
        "        APP_STATE[\"tokenizer\"] = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        logger.success(\"Tiktoken 'cl00k_base' OK.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Tiktoken fail: {e}. Using len().\")\n",
        "        APP_STATE[\"tokenizer\"] = None\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models_ok = True\n",
        "\n",
        "    # Initialize Azure Embeddings Client\n",
        "    if not APP_STATE.get(\"azure_embedding_client\"):\n",
        "        try:\n",
        "            logger.info(f\"Loading Azure Embeddings client: {AZURE_EMBEDDING_MODEL_NAME}...\")\n",
        "            APP_STATE[\"azure_embedding_client\"] = EmbeddingsClient(\n",
        "                endpoint=AZURE_EMBEDDING_ENDPOINT,\n",
        "                credential=AzureKeyCredential(AZURE_EMBEDDING_API_KEY_SECRET)\n",
        "            )\n",
        "            APP_STATE[\"embedding_model\"] = APP_STATE[\"azure_embedding_client\"]\n",
        "            logger.success(f\"Azure Embeddings client for '{AZURE_EMBEDDING_MODEL_NAME}' initialized.\")\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Azure Embeddings client load FAILED: {e}\", exc_info=True)\n",
        "            models_ok = False\n",
        "\n",
        "    if not APP_STATE.get(\"reranker_model\"):\n",
        "        try:\n",
        "            logger.info(f\"Loading reranker: {RERANKER_MODEL_NAME} ({device})...\")\n",
        "            APP_STATE[\"reranker_model\"] = CrossEncoder(\n",
        "                RERANKER_MODEL_NAME,\n",
        "                device=device,\n",
        "                trust_remote_code=True,\n",
        "                max_length=RERANKER_MAX_TOKENS,\n",
        "            )\n",
        "            logger.success(\"Reranker OK.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Reranker load FAIL: {e}. Reranking off.\")\n",
        "            APP_STATE[\"reranker_model\"] = None\n",
        "    APP_STATE[\"rag_models_loaded\"] = models_ok and APP_STATE[\"embedding_model\"] is not None\n",
        "    if not APP_STATE[\"rag_models_loaded\"]:\n",
        "        logger.error(\"Essential RAG models FAILED to load.\")\n",
        "    return APP_STATE[\"rag_models_loaded\"]\n",
        "\n",
        "def initialize_chromadb_from_kb():\n",
        "    global APP_STATE\n",
        "    if APP_STATE[\"db_initialized\"]:\n",
        "        return True\n",
        "    logger.info(f\"Initializing ChromaDB: path='{CHROMA_DB_PATH}'\")\n",
        "    try:\n",
        "        APP_STATE[\"chroma_client\"] = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "        logger.success(f\"ChromaDB persistent OK: '{CHROMA_DB_PATH}'.\")\n",
        "        APP_STATE[\"db_initialized\"] = True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Chroma persistent FAIL: {e}. Trying in-memory.\")\n",
        "        try:\n",
        "            APP_STATE[\"chroma_client\"] = chromadb.Client()\n",
        "            logger.success(\"Chroma in-memory OK.\")\n",
        "            APP_STATE[\"db_initialized\"] = True\n",
        "        except Exception as e_mem:\n",
        "            logger.critical(f\"Chroma in-memory FAIL: {e_mem}\")\n",
        "            APP_STATE[\"db_initialized\"] = False\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_or_create_collection_cached(\n",
        "    collection_name_str: str,\n",
        ") -> Optional[chromadb.api.models.Collection.Collection]:\n",
        "    if not APP_STATE.get(\"db_initialized\") or not APP_STATE.get(\"chroma_client\"):\n",
        "        logger.error(\"ChromaDB not init.\")\n",
        "        return None\n",
        "    if collection_name_str in APP_STATE[\"chroma_collection_cache\"]:\n",
        "        logger.debug(f\"Using cached collection: {collection_name_str}\")\n",
        "        return APP_STATE[\"chroma_collection_cache\"][collection_name_str]\n",
        "    try:\n",
        "        collection = APP_STATE[\"chroma_client\"].get_or_create_collection(\n",
        "            name=collection_name_str, metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "        APP_STATE[\"chroma_collection_cache\"][collection_name_str] = collection\n",
        "        logger.info(\n",
        "            f\"Collection '{collection_name_str}' accessed/created. Items: {collection.count()}\"\n",
        "        )\n",
        "        return collection\n",
        "    except Exception as e:\n",
        "        logger.error(\n",
        "            f\"FAIL get/create Chroma collection '{collection_name_str}': {e}\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "# --- 7. Helper Functions ---\n",
        "def count_tokens(text: str) -> int:\n",
        "    if APP_STATE.get(\"tokenizer\"):\n",
        "        try:\n",
        "            return len(APP_STATE[\"tokenizer\"].encode(text, disallowed_special=()))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Tiktoken encode error for text '{text[:30]}...': {e}. Using char count.\");\n",
        "            return len(text)\n",
        "    return len(text)\n",
        "\n",
        "def image_to_base64_data_uri(pil_image: Image.Image) -> str:\n",
        "    buffered = io.BytesIO()\n",
        "    img_format = pil_image.format if pil_image.format else 'PNG'\n",
        "    try:\n",
        "        pil_image.save(buffered, format=img_format)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Warning: Could not save image in format {img_format}, falling back to PNG. Error: {e}\")\n",
        "        img_format = 'PNG'\n",
        "        pil_image.save(buffered, format=img_format)\n",
        "    encoded_bytes = base64.b64encode(buffered.getvalue())\n",
        "    encoded_string = encoded_bytes.decode('utf-8')\n",
        "    mime_type = Image.MIME.get(img_format.upper(), 'image/png')\n",
        "    return f\"data:{mime_type};base64,{encoded_string}\"\n",
        "\n",
        "def _process_image_with_vision_model(pil_image: Image.Image, prompt_text: str) -> str:\n",
        "    if not APP_STATE.get(\"vision_client_initialized\") or not APP_STATE.get(\n",
        "        \"hf_vision_client\"\n",
        "    ):\n",
        "        logger.error(\"Vision model client not initialized. Cannot process image.\")\n",
        "        return \"Error: Vision model client not initialized.\"\n",
        "    if VISION_MODEL_ID_CONFIG == \"YOUR_NOVITA_SUPPORTED_VISION_MODEL_ID_PLACEHOLDER\":\n",
        "        logger.error(f\"Vision model ID is a placeholder: {VISION_MODEL_ID_CONFIG}. Cannot process image.\")\n",
        "        return \"Error: Vision model ID is a placeholder.\"\n",
        "\n",
        "    base64_image_url = image_to_base64_data_uri(pil_image)\n",
        "    try:\n",
        "        completion = APP_STATE[\"hf_vision_client\"].chat.completions.create(\n",
        "            model=VISION_MODEL_ID_CONFIG,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}, {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image_url}}]}\n",
        "            ],\n",
        "            max_tokens=1500\n",
        "        )\n",
        "        if completion.choices and completion.choices[0].message and completion.choices[0].message.content:\n",
        "            return completion.choices[0].message.content.strip()\n",
        "        else:\n",
        "            logger.error(f\"Vision model ({VISION_MODEL_ID_CONFIG}) - No content in response. Prompt: {prompt_text[:50]}...\")\n",
        "            return f\"Error: No content from vision model {VISION_MODEL_ID_CONFIG}.\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred with the vision model ({VISION_MODEL_ID_CONFIG}): {e}\", exc_info=True)\n",
        "        return f\"Error processing image with vision model {VISION_MODEL_ID_CONFIG}: {str(e)}\"\n",
        "\n",
        "def sanitize_chromadb_collection_name(name: str) -> str:\n",
        "    name = str(name)\n",
        "    name = re.sub(r'[ \\t\\n\\r\\f\\v.,;:!?\"\\'`()\\[\\]{}<>|/\\\\]+', '_', name)\n",
        "    name = re.sub(r'[^a-zA-Z0-9_-]', '', name)\n",
        "    name = re.sub(r'^_+|-+$', '', name)\n",
        "    name = re.sub(r'^-+|_+$', '', name)\n",
        "    if name and not name[0].isalnum():\n",
        "        name = 'c_' + name\n",
        "    if name and not name[-1].isalnum():\n",
        "        name = name + '_c'\n",
        "    if len(name) < 3:\n",
        "        name = name + '___'\n",
        "        name = name[:3]\n",
        "    if len(name) > 63:\n",
        "        name = name[:63]\n",
        "    if name and not name[0].isalnum(): name = 'c' + name[1:]\n",
        "    if name and len(name) > 1 and not name[-1].isalnum(): name = name[:-1] + 'c'\n",
        "    elif name and len(name) == 1 and not name[0].isalnum(): name = 'cc'\n",
        "    if not name or len(name) < 3:\n",
        "        name = f\"coll_{uuid.uuid4().hex[:8]}\"\n",
        "    return name.lower()\n",
        "\n",
        "\n",
        "# --- 8. PDF Parsing Logic (The CORE parser for all converted PDFs) ---\n",
        "def get_font_properties(span_dict: Dict) -> Dict:\n",
        "    return {\n",
        "        \"size\": span_dict.get(\"size\", 0.0),\n",
        "        \"font\": span_dict.get(\"font\", \"UnknownFont\"),\n",
        "        \"color\": span_dict.get(\"color\", 0),\n",
        "        \"flags\": span_dict.get(\"flags\", 0),\n",
        "        \"origin_x\": span_dict.get(\"bbox\", [0, 0, 0, 0])[0],\n",
        "        \"origin_y\": span_dict.get(\"bbox\", [0, 0, 0, 0])[1],\n",
        "    }\n",
        "\n",
        "def is_likely_header_or_footer(\n",
        "    block_text: str,\n",
        "    page_rect: fitz.Rect,\n",
        "    block_bbox: fitz.Rect,\n",
        "    page_number: int,\n",
        "    num_pages: int,\n",
        ") -> bool:\n",
        "    block_content = block_text.strip()\n",
        "    if not block_content or len(block_content) > 150:\n",
        "        return False\n",
        "    page_height = page_rect.height\n",
        "    is_top_zone = block_bbox.y1 < page_rect.y0 + 0.12 * page_height\n",
        "    is_bottom_zone = block_bbox.y0 > page_rect.y1 - 0.12 * page_height\n",
        "    if not (is_top_zone or is_bottom_zone):\n",
        "        return False\n",
        "    page_num_str = str(page_number)\n",
        "    num_pages_str = str(num_pages)\n",
        "    patterns = [\n",
        "        r\"^(Page\\s*)?\" + re.escape(page_num_str) + r\"(\\s*(of|-|/)\\s*\" + re.escape(num_pages_str) + r\")?([\\s.]*)$\",\n",
        "        r\"^(\\[?\" + re.escape(page_num_str) + r\"\\]?)$\",\n",
        "        r\"^\\s*-\\s*\\d+\\s*-\\s*$\",\n",
        "        r\"^\\s*\" + re.escape(page_num_str) + r\"\\s*$\",\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        if re.fullmatch(pattern, block_content, re.IGNORECASE):\n",
        "            return True\n",
        "    if len(block_content.split()) < 7 and len(block_content) < 70:\n",
        "        if not re.search(r\"[.!?]$\", block_content):\n",
        "            if block_content.isupper() or block_content.istitle():\n",
        "                return True\n",
        "            if len(block_content) < 10 and len(set(block_content.replace(\" \", \"\"))) < 4:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def parse_pdf_content_worker(pdf_path: str, original_filename: str, original_document_type: str, config_dict: dict) -> list:\n",
        "    logger.info(f\"  Worker: Parsing PDF content from '{os.path.basename(pdf_path)}' (originally {original_document_type}) with config: {config_dict}\")\n",
        "    all_content_blocks = []\n",
        "    doc_block_counter = 0\n",
        "\n",
        "    use_pdfplumber_tables = config_dict.get(\"use_pdfplumber_tables\", True)\n",
        "    use_pymupdf_text = config_dict.get(\"use_pymupdf_text\", True)\n",
        "    process_scanned_pages = config_dict.get(\"process_scanned_pages\", False)\n",
        "    use_vision_for_ocr_flag = config_dict.get(\"use_vision_for_ocr\", False)\n",
        "    process_structured_images = config_dict.get(\"process_structured_images\", False)\n",
        "    use_vision_for_description_flag = config_dict.get(\"use_vision_for_description\", False)\n",
        "    scan_detection_char_threshold = config_dict.get(\"scan_detection_char_threshold\", 100)\n",
        "    dpi_for_conversion = config_dict.get(\"dpi_for_conversion\", 200)\n",
        "\n",
        "    tables_by_page = {}\n",
        "    if use_pdfplumber_tables:\n",
        "        logger.debug(\"  Worker: Attempting table extraction with PdfPlumber...\")\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf_pl:\n",
        "                for p_idx, page_pl in enumerate(pdf_pl.pages):\n",
        "                    page_num_human = p_idx + 1\n",
        "                    raw_tables = page_pl.find_tables()\n",
        "                    extracted_tables_data = []\n",
        "                    if raw_tables:\n",
        "                        logger.debug(f\"    P{page_num_human}: Found {len(raw_tables)} potential tables with PdfPlumber.\")\n",
        "                        for tbl_idx, raw_table in enumerate(raw_tables):\n",
        "                            md_table = f\"[Table {tbl_idx+1} on Page {page_num_human}]\\n\"\n",
        "                            data = raw_table.extract()\n",
        "                            if data:\n",
        "                                try:\n",
        "                                    header = data[0]\n",
        "                                    if header and all(isinstance(c, (str, type(None))) for c in header):\n",
        "                                        md_table += \"| \" + \" | \".join(str(c).strip().replace(\"\\n\", \" \") if c is not None else \"\" for c in header) + \" |\\n\"\n",
        "                                        md_table += \"| \" + \" | \".join(\"---\" for _ in header) + \" |\\n\"\n",
        "                                        for row in data[1:]:\n",
        "                                            if row and all(isinstance(c, (str, type(None))) for c in row):\n",
        "                                                md_table += f\"| {' | '.join(str(c).strip().replace(chr(10),' ') if c is not None else '' for c in row)} |\\n\"\n",
        "                                    else:\n",
        "                                        for row_idx, row in enumerate(data):\n",
        "                                            if row and all(isinstance(c, (str, type(None))) for c in row):\n",
        "                                                md_table += (\"| \" if row_idx == 0 else \"\") + \" | \".join(str(c).strip().replace(\"\\n\", \" \") if c is not None else \"\" for c in row) + (\" |\\n\" if row_idx == 0 and len(data) > 1 else \"\\n\")\n",
        "                                                if row_idx == 0 and len(data) > 1:\n",
        "                                                    md_table += \"| \" + \" | \".join(\"---\" for _ in row) + \" |\\n\"\n",
        "                                except TypeError:\n",
        "                                    md_table += \"(Complex table structure, basic markdown conversion failed)\\n\"\n",
        "                                    logger.debug(f\"    P{page_num_human} T{tbl_idx+1}: TypeError during MD conversion.\")\n",
        "                            else:\n",
        "                                md_table += \"(Table detected, but no data could be extracted or table is empty)\\n\"\n",
        "                            extracted_tables_data.append(\n",
        "                                {\"bbox\": raw_table.bbox, \"markdown_content\": md_table, \"order\": raw_table.bbox[1]}\n",
        "                            )\n",
        "                    if extracted_tables_data:\n",
        "                        tables_by_page[page_num_human] = sorted(\n",
        "                            extracted_tables_data, key=lambda t: t[\"order\"]\n",
        "                        )\n",
        "        except Exception as e:\n",
        "            logger.error(\n",
        "                f\"  Worker: Pdfplumber table extraction FAILED for '{os.path.basename(pdf_path)}': {e}\",\n",
        "                exc_info=True,\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"  Worker: PyMuPDF open FAILED for '{os.path.basename(pdf_path)}': {e}.\")\n",
        "        return [{\"block_id\":\"error_doc_open\", \"type\":\"error\", \"content\":f\"PDF open fail: {e}\", \"page_number\":0, \"document_type\": original_document_type}]\n",
        "\n",
        "    num_pages = len(doc)\n",
        "    all_font_sizes_doc = []\n",
        "    logger.debug(f\"  Worker: Document '{os.path.basename(pdf_path)}' has {num_pages} pages. Calculating font statistics...\")\n",
        "    for page_fitz_temp in doc:\n",
        "        try:\n",
        "            textpage_temp = page_fitz_temp.get_textpage_ocr(flags=0, full=False)\n",
        "            blocks_dict_temp = textpage_temp.extractDICT().get(\"blocks\", [])\n",
        "            for block_dict_temp in blocks_dict_temp:\n",
        "                if block_dict_temp.get(\"type\") == 0:\n",
        "                    for line_dict_temp in block_dict_temp.get(\"lines\", []):\n",
        "                        for span_dict_temp in line_dict_temp.get(\"spans\", []):\n",
        "                            all_font_sizes_doc.append(span_dict_temp.get(\"size\", 0))\n",
        "        except Exception as e_fs:\n",
        "            logger.warning(f\"    Font size stats extraction error on page {page_fitz_temp.number + 1}: {e_fs}\")\n",
        "\n",
        "    avg_font_size = np.mean([s for s in all_font_sizes_doc if s > 0]) if any(s > 0 for s in all_font_sizes_doc) else 10.0\n",
        "    std_font_size = np.std([s for s in all_font_sizes_doc if s > 0]) if any(s > 0 for s in all_font_sizes_doc) and len(set(s for s in all_font_sizes_doc if s > 0)) > 1 else 2.0\n",
        "    if std_font_size < 1.5:\n",
        "        std_font_size = 1.5\n",
        "    logger.info(f\"  Doc stats: Pages:{num_pages}, AvgFont:{avg_font_size:.1f}, StdFont:{std_font_size:.1f}\")\n",
        "\n",
        "    for page_idx, page_fitz in enumerate(doc):\n",
        "        page_num_human = page_idx + 1\n",
        "        logger.debug(f\"  Worker: Processing Page {page_num_human}/{num_pages}\")\n",
        "        page_content_elements = []\n",
        "        current_page_table_bboxes = [\n",
        "            tuple(t[\"bbox\"]) for t in tables_by_page.get(page_num_human, [])\n",
        "        ]\n",
        "\n",
        "        def check_overlap_with_tables(b_bbox_coords, t_bboxes_coords_list, threshold=0.3):\n",
        "            b_x0,b_y0,b_x1,b_y1 = b_bbox_coords\n",
        "            b_area=(b_x1-b_x0)*(b_y1-b_y0)\n",
        "            if b_area == 0: return False\n",
        "            for t_coords in t_bboxes_coords_list:\n",
        "                t_x0,t_y0,t_x1,t_y1 = t_coords\n",
        "                ix0,iy0 = max(b_x0,t_x0), max(b_y0,t_y0)\n",
        "                ix1,iy1 = min(b_x1,t_x1), min(b_y1,t_y1)\n",
        "                iarea = max(0,ix1-ix0) * max(0,iy1-iy0)\n",
        "                if (iarea/b_area) > threshold: return True\n",
        "            return False\n",
        "\n",
        "        if use_pymupdf_text:\n",
        "            try:\n",
        "                textpage_flags = (\n",
        "                    fitz.TEXTFLAGS_SEARCH | fitz.TEXTFLAGS_PRESERVE_LIGATURES\n",
        "                    | fitz.TEXTFLAGS_PRESERVE_IMAGES | fitz.TEXTFLAGS_PRESERVE_WHITESPACE\n",
        "                )\n",
        "            except AttributeError:\n",
        "                textpage_flags = fitz.TEXTFLAGS_SEARCH\n",
        "                logger.warning(f\"    P{page_num_human}: Older PyMuPDF version, using basic text flags.\")\n",
        "\n",
        "            try:\n",
        "                textpage = page_fitz.get_textpage_ocr(flags=textpage_flags, full=False)\n",
        "                page_dict = textpage.extractDICT()\n",
        "\n",
        "                for block_dict in page_dict.get(\"blocks\", []):\n",
        "                    if block_dict.get(\"type\") == 0:\n",
        "                        block_text_content, span_font_props_list = \"\", []\n",
        "                        for line_dict in block_dict.get(\"lines\", []):\n",
        "                            line_text_parts = []\n",
        "                            for span_dict_val in line_dict.get(\"spans\", []):\n",
        "                                line_text_parts.append(span_dict_val[\"text\"])\n",
        "                                span_font_props_list.append(get_font_properties(span_dict_val))\n",
        "                            block_text_content += \" \".join(line_text_parts).strip() + \"\\n\"\n",
        "\n",
        "                        block_text_content = re.sub(r'\\s*\\n\\s*', '\\n', block_text_content).strip()\n",
        "                        block_text_content = re.sub(r' +', ' ', block_text_content)\n",
        "                        block_bbox_fitz = fitz.Rect(block_dict[\"bbox\"])\n",
        "\n",
        "                        if not block_text_content or check_overlap_with_tables(tuple(block_bbox_fitz), current_page_table_bboxes):\n",
        "                            continue\n",
        "\n",
        "                        current_block_avg_font_size = np.mean([s['size'] for s in span_font_props_list if s['size'] > 0]) if any(s['size'] > 0 for s in span_font_props_list) else avg_font_size\n",
        "                        is_bold = any(s['flags'] & (1 << 4) for s in span_font_props_list)\n",
        "                        num_words_first_line = len(block_text_content.split('\\n')[0].split())\n",
        "\n",
        "                        block_sem_type, is_heading_candidate = \"text_paragraph\", False\n",
        "                        if is_likely_header_or_footer(block_text_content, page_fitz.rect, block_bbox_fitz, page_num_human, num_pages):\n",
        "                            block_sem_type = \"noise_header_footer\"\n",
        "                        elif len(block_text_content) <= 6 and not re.search(r'[a-zA-Z]{2,}', block_text_content) and \\\n",
        "                             not (current_block_avg_font_size > avg_font_size + 0.8 * std_font_size or is_bold):\n",
        "                            block_sem_type = \"noise_short_irrelevant\"\n",
        "                        else:\n",
        "                            if page_idx == 0 and current_block_avg_font_size >= avg_font_size + 1.8 * std_font_size and \\\n",
        "                               num_words_first_line < 18 and block_bbox_fitz.y0 < page_fitz.rect.height * 0.3:\n",
        "                                block_sem_type, is_heading_candidate = \"title_document\", True\n",
        "                            elif current_block_avg_font_size >= avg_font_size + 1.4 * std_font_size and \\\n",
        "                                 (is_bold or num_words_first_line < 20 or (page_idx <=1 and num_words_first_line < 28)):\n",
        "                                block_sem_type, is_heading_candidate = \"h1_heading\", True\n",
        "                            elif current_block_avg_font_size >= avg_font_size + 0.7 * std_font_size and \\\n",
        "                                 (is_bold or num_words_first_line < 25):\n",
        "                                block_sem_type, is_heading_candidate = \"h2_heading\", True\n",
        "                            elif (current_block_avg_font_size >= avg_font_size + 0.3 * std_font_size and \\\n",
        "                                  is_bold and num_words_first_line < 30 and not block_text_content.endswith('.')):\n",
        "                                block_sem_type, is_heading_candidate = \"h3_heading\", True\n",
        "\n",
        "                        if is_heading_candidate:\n",
        "                            logger.trace(f\"    P{page_num_human}: Found {block_sem_type} (F:{current_block_avg_font_size:.1f},B:{is_bold}): '{block_text_content[:40]}...'\")\n",
        "                        page_content_elements.append({\n",
        "                            \"type\": block_sem_type, \"content\": block_text_content, \"bbox\": tuple(block_bbox_fitz),\n",
        "                            \"order\": block_bbox_fitz.y0, \"is_semantic_heading\": is_heading_candidate,\n",
        "                            \"font_size\": current_block_avg_font_size, \"is_bold\": is_bold, \"document_type\": original_document_type\n",
        "                        })\n",
        "            except Exception as e_pymupdf_txt:\n",
        "                logger.error(f\"  Worker: PyMuPDF text extraction error on P{page_num_human}: {e_pymupdf_txt}\", exc_info=True)\n",
        "\n",
        "        for tbl_data in tables_by_page.get(page_num_human, []):\n",
        "            page_content_elements.append({\n",
        "                \"type\": \"table_markdown\", \"content\": tbl_data[\"markdown_content\"],\n",
        "                \"bbox\": tbl_data[\"bbox\"], \"order\": tbl_data[\"order\"],\n",
        "                \"is_semantic_heading\": False, \"font_size\": avg_font_size, \"is_bold\": False, \"document_type\": original_document_type\n",
        "            })\n",
        "\n",
        "        initial_digital_text_len = sum(len(el[\"content\"]) for el in page_content_elements if el[\"type\"] not in [\"noise_header_footer\", \"noise_short_irrelevant\"])\n",
        "        page_might_be_scan = initial_digital_text_len < scan_detection_char_threshold\n",
        "\n",
        "        structured_images_on_page_meta = []\n",
        "        if process_scanned_pages or process_structured_images:\n",
        "            try:\n",
        "                structured_images_on_page_meta = page_fitz.get_images(full=True)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"  Worker: PyMuPDF get_images failed P{page_num_human}: {e}\")\n",
        "\n",
        "        if process_scanned_pages and page_might_be_scan and not structured_images_on_page_meta:\n",
        "            logger.info(f\"  P{page_num_human}: Low digital text ({initial_digital_text_len} chars), no structured images. Candidate for full page OCR.\")\n",
        "            try:\n",
        "                pil_page_img_list = convert_from_path(\n",
        "                    pdf_path, dpi=dpi_for_conversion, first_page=page_num_human,\n",
        "                    last_page=page_num_human, fmt='jpeg', thread_count=1\n",
        "                )\n",
        "                if not pil_page_img_list:\n",
        "                    logger.warning(f\"    P{page_num_human}: pdf2image returned no image for full page OCR.\")\n",
        "                    continue\n",
        "                pil_page_img = pil_page_img_list[0]\n",
        "                fp_bbox, fp_ocr_text, fp_ocr_type_detail = tuple(page_fitz.rect), \"\", \"\"\n",
        "\n",
        "                if use_vision_for_ocr_flag and APP_STATE.get(\"hf_vision_client\"):\n",
        "                    logger.debug(f\"    P{page_num_human}: Attempting Vision OCR (full page).\")\n",
        "                    fp_ocr_text = _process_image_with_vision_model(pil_page_img, IMAGE_OCR_PROMPT_CONFIG)\n",
        "                    fp_ocr_type_detail = \"vision_ocr (full_page)\"\n",
        "\n",
        "                if not fp_ocr_text.strip() or \"Error:\" in fp_ocr_text or \"No text found\" in fp_ocr_text.lower():\n",
        "                    logger.debug(f\"    P{page_num_human}: Vision OCR failed or no text. Fallback/Attempt Tesseract (full page). Vision out: '{fp_ocr_text[:50]}...'\")\n",
        "                    if not use_vision_for_ocr_flag or (\"Error:\" in fp_ocr_text or \"No text found\" in fp_ocr_text.lower()):\n",
        "                        fp_ocr_text = pytesseract.image_to_string(pil_page_img)\n",
        "                        fp_ocr_type_detail = \"tesseract_ocr (full_page)\"\n",
        "\n",
        "                if fp_ocr_text and fp_ocr_text.strip() and \"Error:\" not in fp_ocr_text and \"No text found\" not in fp_ocr_text.lower():\n",
        "                    page_content_elements = [el for el in page_content_elements if el[\"type\"] == \"table_markdown\"]\n",
        "                    page_content_elements.append({\n",
        "                        \"type\": \"full_page_ocr_text\", \"content\": fp_ocr_text.strip(), \"bbox\": fp_bbox,\n",
        "                        \"order\": 0, \"is_semantic_heading\": False, \"font_size\": avg_font_size,\n",
        "                        \"is_bold\": False, \"ocr_source\": fp_ocr_type_detail, \"document_type\": original_document_type\n",
        "                    })\n",
        "                    logger.info(f\"    P{page_num_human}: Full scan OCR successful via {fp_ocr_type_detail}. Text length: {len(fp_ocr_text)}\")\n",
        "                else:\n",
        "                    logger.warning(f\"    P{page_num_human}: Full page OCR attempt ({fp_ocr_type_detail or 'N/A'}) yielded no usable text.\")\n",
        "            except Exception as e_fp_ocr:\n",
        "                logger.error(f\"    Error during P{page_num_human} full page OCR processing: {e_fp_ocr}\", exc_info=True)\n",
        "\n",
        "        if process_structured_images and structured_images_on_page_meta:\n",
        "            logger.debug(f\"  P{page_num_human}: Processing {len(structured_images_on_page_meta)} structured image regions.\")\n",
        "            for img_idx, img_meta_item in enumerate(structured_images_on_page_meta):\n",
        "                xref = img_meta_item[0]\n",
        "                try:\n",
        "                    base_image = doc.extract_image(xref)\n",
        "                    if not base_image or \"image\" not in base_image:\n",
        "                        logger.warning(f\"    P{page_num_human} ImgX{xref}: Could not extract base image data.\")\n",
        "                        continue\n",
        "\n",
        "                    pil_img = Image.open(io.BytesIO(base_image[\"image\"]))\n",
        "                    img_placements_rects = page_fitz.get_image_rects(xref)\n",
        "\n",
        "                    if not img_placements_rects:\n",
        "                         if len(structured_images_on_page_meta) == 1 and page_might_be_scan:\n",
        "                             img_placements_rects = [page_fitz.rect]\n",
        "                             logger.debug(\"    Single image on scan-like page, using full page rect.\")\n",
        "                         else:\n",
        "                             logger.warning(f\"    P{page_num_human} ImgX{xref}: No placement rectangles found. Skipping image.\")\n",
        "                             continue\n",
        "\n",
        "                    for rect_idx, fitz_rect_obj in enumerate(img_placements_rects):\n",
        "                        img_bbox_coords = tuple(fitz_rect_obj)\n",
        "                        img_filename_ref = f\"p{page_num_human}_img{img_idx}_xref{xref}_r{rect_idx}.{base_image.get('ext','png')}\"\n",
        "                        if check_overlap_with_tables(img_bbox_coords, current_page_table_bboxes, 0.7):\n",
        "                            logger.trace(f\"    Skipping image XREF {xref} Rect {rect_idx} due to high table overlap.\")\n",
        "                            continue\n",
        "\n",
        "                        ocr_from_img_content, desc_from_img_content, vision_ocr_attempted_for_img = \"\", \"\", False\n",
        "                        if use_vision_for_ocr_flag and APP_STATE.get(\"hf_vision_client\"):\n",
        "                            vision_ocr_attempted_for_img = True\n",
        "                            ocr_from_img_content = _process_image_with_vision_model(pil_img, IMAGE_OCR_PROMPT_CONFIG)\n",
        "                            if ocr_from_img_content and \"Error:\" not in ocr_from_img_content and \"No text found\" not in ocr_from_img_content.lower():\n",
        "                                page_content_elements.append({\n",
        "                                    \"type\": \"image_ocr_text\", \"content\": ocr_from_img_content.strip(),\n",
        "                                    \"bbox\": img_bbox_coords, \"order\": img_bbox_coords[1],\n",
        "                                    \"source_image_filename\": img_filename_ref, \"is_semantic_heading\": False,\n",
        "                                    \"font_size\": avg_font_size, \"is_bold\": False, \"document_type\": original_document_type\n",
        "                                })\n",
        "                            elif \"No text found\" in ocr_from_img_content.lower(): ocr_from_img_content = \"\"\n",
        "                            elif \"Error:\" in ocr_from_img_content:\n",
        "                                logger.warning(f\"      P{page_num_human},ImgX{xref}R{rect_idx} Vision OCR Error: {ocr_from_img_content}\")\n",
        "                                ocr_from_img_content = \"\"\n",
        "\n",
        "                        if process_structured_images and use_vision_for_description_flag and APP_STATE.get(\"hf_vision_client\"):\n",
        "                            should_describe_img = not (\n",
        "                                vision_ocr_attempted_for_img and len(ocr_from_img_content) > 75 and\n",
        "                                not any(k in ocr_from_img_content.lower() for k in [\"chart\",\"diagram\",\"graph\",\"figure\",\"table\"])\n",
        "                            )\n",
        "                            if should_describe_img:\n",
        "                                desc_from_img_content = _process_image_with_vision_model(pil_img, IMAGE_DESCRIPTION_PROMPT_CONFIG)\n",
        "                                if desc_from_img_content and \"Error:\" not in desc_from_img_content and desc_from_img_content.strip():\n",
        "                                    if not(ocr_from_img_content and desc_from_img_content.strip().lower() == ocr_from_img_content.strip().lower()):\n",
        "                                        page_content_elements.append({\n",
        "                                            \"type\": \"image_description\", \"content\": desc_from_img_content.strip(),\n",
        "                                            \"bbox\": img_bbox_coords, \"order\": img_bbox_coords[1] + 0.01,\n",
        "                                            \"source_image_filename\": img_filename_ref, \"is_semantic_heading\": False,\n",
        "                                            \"font_size\": avg_font_size, \"is_bold\": False, \"document_type\": original_document_type\n",
        "                                        })\n",
        "                except UnidentifiedImageError:\n",
        "                    logger.warning(f\"  P{page_num_human}, ImgX{xref}: PIL UnidentifiedImageError. Skipping this image resource.\")\n",
        "                except Exception as e_img_proc:\n",
        "                    logger.error(f\"  Error processing structured image XREF {xref} on P{page_num_human}: {e_img_proc}\", exc_info=True)\n",
        "\n",
        "\n",
        "        page_content_elements.sort(key=lambda item: item.get(\"order\", float('inf')))\n",
        "        for el_data in page_content_elements:\n",
        "            content, el_type_str = el_data.get(\"content\", \"\").strip(), el_data.get(\"type\", \"\")\n",
        "            if \"noise\" in el_type_str or content.lower() == \"no text found\":\n",
        "                logger.trace(f\"    P{page_num_human}: Final skip of noise block type '{el_type_str}' or 'No text found'.\")\n",
        "                continue\n",
        "            if content:\n",
        "                doc_block_counter += 1\n",
        "                block_to_add = {k: v for k, v in el_data.items() if k not in [\"order\"]}\n",
        "                block_to_add.update({\"block_id\": f\"doc_block_{doc_block_counter}\", \"page_number\": page_num_human})\n",
        "                block_to_add.setdefault(\"is_semantic_heading\", False)\n",
        "                block_to_add.setdefault(\"font_size\", avg_font_size)\n",
        "                block_to_add.setdefault(\"is_bold\", False)\n",
        "                block_to_add.setdefault(\"document_type\", original_document_type)\n",
        "                all_content_blocks.append(block_to_add)\n",
        "    try:\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"  Worker: Error closing PDF '{os.path.basename(pdf_path)}': {e}\")\n",
        "\n",
        "    if not all_content_blocks:\n",
        "        all_content_blocks.append({\n",
        "            \"block_id\": \"fallback_empty_doc\", \"type\": \"info\",\n",
        "            \"content\": f\"No content could be extracted from this {original_document_type} file after PDF conversion. It might be empty, purely graphical without OCR, or a protected file.\",\n",
        "            \"page_number\": 0, \"document_type\": original_document_type\n",
        "        })\n",
        "    logger.info(\n",
        "        f\"  Worker: Finished parsing '{os.path.basename(pdf_path)}' (originally {original_document_type}). Total blocks extracted: {len(all_content_blocks)}\"\n",
        "    )\n",
        "    return all_content_blocks\n",
        "\n",
        "# --- NEW: Universal File Converter to PDF ---\n",
        "def convert_to_pdf(input_path: str, temp_dir: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Converts various document types (DOCX, XLSX, PPTX, Image) to PDF using unoconv or Pillow.\n",
        "    Returns a dictionary with {pdf_path, original_filename, original_extension, original_document_type}.\n",
        "    \"\"\"\n",
        "    original_filename = os.path.basename(input_path)\n",
        "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
        "    base_name_no_ext = os.path.splitext(original_filename)[0]\n",
        "    output_pdf_path = os.path.join(temp_dir, f\"{base_name_no_ext}_{uuid.uuid4().hex[:6]}.pdf\")\n",
        "\n",
        "    original_doc_type = \"unknown\"\n",
        "    if file_extension == \".pdf\":\n",
        "        original_doc_type = \"pdf\"\n",
        "    elif file_extension == \".docx\":\n",
        "        original_doc_type = \"docx\"\n",
        "    elif file_extension == \".xlsx\":\n",
        "        original_doc_type = \"xlsx\"\n",
        "    elif file_extension == \".pptx\":\n",
        "        original_doc_type = \"pptx\"\n",
        "    elif file_extension in [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\"]:\n",
        "        original_doc_type = \"image\"\n",
        "    elif file_extension == \".xls\": # Old Excel format, map to xlsx for parsing\n",
        "        original_doc_type = \"xlsx\" # Treat as xlsx for parsing context after conversion\n",
        "    else:\n",
        "        logger.error(f\"Conversion failed: Unsupported file type for conversion: {file_extension} for '{original_filename}'\")\n",
        "        return {\"pdf_path\": None, \"original_filename\": original_filename, \"original_extension\": file_extension, \"original_document_type\": \"unsupported\", \"error\": f\"Unsupported file type: {file_extension}\"}\n",
        "\n",
        "    if file_extension == \".pdf\":\n",
        "        logger.info(f\"  File '{original_filename}' is already a PDF. Skipping conversion.\")\n",
        "        return {\n",
        "            \"pdf_path\": input_path,\n",
        "            \"original_filename\": original_filename,\n",
        "            \"original_extension\": file_extension,\n",
        "            \"original_document_type\": original_doc_type\n",
        "        }\n",
        "    elif original_doc_type == \"image\":\n",
        "        try:\n",
        "            img = Image.open(input_path)\n",
        "            if img.mode != \"RGB\":\n",
        "                img = img.convert(\"RGB\")\n",
        "            img.save(output_pdf_path, \"PDF\")\n",
        "            logger.info(f\"  Converted image '{original_filename}' to PDF at '{output_pdf_path}'.\")\n",
        "            return {\n",
        "                \"pdf_path\": output_pdf_path,\n",
        "                \"original_filename\": original_filename,\n",
        "                \"original_extension\": file_extension,\n",
        "                \"original_document_type\": original_doc_type\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"  Failed to convert image '{original_filename}' to PDF using Pillow: {e}\", exc_info=True)\n",
        "            return None\n",
        "    else: # Use unoconv for DOCX, XLSX, PPTX, and now .xls\n",
        "        command = f\"unoconv -f pdf -o {output_pdf_path} {input_path}\"\n",
        "        logger.info(f\"  Attempting to convert '{original_filename}' to PDF using unoconv: {command}\")\n",
        "        try:\n",
        "            result = os.system(command)\n",
        "            if result == 0 and os.path.exists(output_pdf_path) and os.path.getsize(output_pdf_path) > 0:\n",
        "                logger.success(f\"  Successfully converted '{original_filename}' to PDF at '{output_pdf_path}'.\")\n",
        "                return {\n",
        "                    \"pdf_path\": output_pdf_path,\n",
        "                    \"original_filename\": original_filename,\n",
        "                    \"original_extension\": file_extension,\n",
        "                    \"original_document_type\": original_doc_type\n",
        "                }\n",
        "            else:\n",
        "                logger.error(f\"  Unoconv failed or produced empty PDF for '{original_filename}'. Result code: {result}. PDF exists: {os.path.exists(output_pdf_path)}. Output file size: {os.path.getsize(output_pdf_path) if os.path.exists(output_pdf_path) else 'N/A'}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"  Error during unoconv conversion of '{original_filename}': {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "# --- Central Parsing Function (now orchestrates conversion and then PDF parsing) ---\n",
        "def parse_document_via_profile(file_path: str, profile_name: str = \"default_fallback\") -> list:\n",
        "    logger.info(f\"Initiating parsing process for document '{os.path.basename(file_path)}' with profile: '{profile_name}'\")\n",
        "\n",
        "    current_config = {\n",
        "        \"use_pdfplumber_tables\": True, \"use_pymupdf_text\": True, \"process_scanned_pages\": False,\n",
        "        \"process_structured_images\": False, \"use_vision_for_ocr\": False, \"use_vision_for_description\": False,\n",
        "        \"scan_detection_char_threshold\": 120, \"dpi_for_conversion\": 220\n",
        "    }\n",
        "    if profile_name == \"fastest\":\n",
        "        current_config.update({\"process_scanned_pages\": False, \"process_structured_images\": False, \"use_vision_for_ocr\": False, \"use_vision_for_description\": False})\n",
        "    elif profile_name == \"digital_plus_ocr\":\n",
        "        current_config.update({\"process_scanned_pages\": True, \"use_vision_for_ocr\": False, \"process_structured_images\": True, \"use_vision_for_description\": False, \"scan_detection_char_threshold\": 150, \"dpi_for_conversion\": 200})\n",
        "    elif profile_name == \"comprehensive_vision\":\n",
        "        current_config.update({\"process_scanned_pages\": True, \"use_vision_for_ocr\": True, \"process_structured_images\": True, \"use_vision_for_description\": True, \"scan_detection_char_threshold\": 100, \"dpi_for_conversion\": 250})\n",
        "    elif profile_name == \"default_fallback\":\n",
        "        logger.warning(f\"Using 'default_fallback' profile for '{os.path.basename(file_path)}'.\")\n",
        "        current_config.update({\"process_scanned_pages\": True, \"use_vision_for_ocr\": True, \"process_structured_images\": True, \"use_vision_for_description\": True})\n",
        "    else:\n",
        "        logger.error(f\"Unexpected profile '{profile_name}'. Using minimal config.\")\n",
        "        current_config = {\"use_pdfplumber_tables\": True, \"use_pymupdf_text\": True}\n",
        "\n",
        "    if not APP_STATE.get(\"vision_client_initialized\", False):\n",
        "        if current_config.get(\"use_vision_for_ocr\"):\n",
        "            logger.info(f\"Profile '{profile_name}': Vision OCR globally disabled (client not ready).\")\n",
        "            current_config[\"use_vision_for_ocr\"] = False\n",
        "        if current_config.get(\"use_vision_for_description\"):\n",
        "            logger.info(f\"Profile '{profile_name}': Vision Desc globally disabled (client not ready).\")\n",
        "            current_config[\"use_vision_for_description\"] = False\n",
        "\n",
        "    logger.info(f\"Final config for parsing '{os.path.basename(file_path)}' (profile '{profile_name}'): {current_config}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    converted_info = convert_to_pdf(file_path, STAGED_UPLOADS_DIR) # Use staged uploads dir for temp PDFs\n",
        "    if not converted_info or not converted_info[\"pdf_path\"]:\n",
        "        error_msg = converted_info.get(\"error\", \"Unknown conversion error.\")\n",
        "        logger.error(f\"Document conversion failed for '{os.path.basename(file_path)}': {error_msg}\")\n",
        "        return [{\"block_id\":\"conversion_error\", \"type\":\"error\", \"content\":f\"File conversion to PDF failed: {error_msg}\", \"page_number\":0, \"document_type\": converted_info.get(\"original_document_type\", \"unsupported\")}]\n",
        "\n",
        "    pdf_to_parse_path = converted_info[\"pdf_path\"]\n",
        "    original_filename = converted_info[\"original_filename\"]\n",
        "    original_document_type = converted_info[\"original_document_type\"]\n",
        "\n",
        "    try:\n",
        "        # Now call the single PDF content parser\n",
        "        parsed_data = parse_pdf_content_worker(pdf_to_parse_path, original_filename, original_document_type, current_config)\n",
        "\n",
        "        # Ensure that blocks from non-PDFs have their `document_type` consistently set\n",
        "        for block in parsed_data:\n",
        "            block[\"document_type\"] = original_document_type\n",
        "\n",
        "        logger.success(f\"Profile '{profile_name}' parsing for '{original_filename}' (converted from {original_document_type}) in {time.time()-start_time:.2f}s. Blocks: {len(parsed_data)}.\")\n",
        "        return parsed_data\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during PDF content parsing of converted file '{pdf_to_parse_path}' (originally {original_document_type}): {e}\", exc_info=True)\n",
        "        return [{\"block_id\":\"parsing_error\", \"type\":\"error\", \"content\":f\"Error parsing PDF content after conversion: {e}\", \"page_number\":0, \"document_type\": original_document_type}]\n",
        "    finally:\n",
        "        # Clean up the temporary PDF file if it was created\n",
        "        if pdf_to_parse_path != file_path: # Only remove if it's a new temp file, not the original if it was already a PDF\n",
        "            try:\n",
        "                os.remove(pdf_to_parse_path)\n",
        "                logger.info(f\"  Removed temporary PDF file: '{pdf_to_parse_path}'.\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"  Could not remove temporary PDF file '{pdf_to_parse_path}': {e}\")\n",
        "\n",
        "\n",
        "# --- 9. Knowledge Base Functions ---\n",
        "def _prepend_context_to_chunk(original_chunk_text: str, block_metadata: dict) -> str:\n",
        "    \"\"\"\n",
        "    Dynamically prepends contextual information to a chunk based on its type and metadata.\n",
        "    Refined to use the actual `document_type` metadata.\n",
        "    \"\"\"\n",
        "    orig_filename = APP_STATE.get(\"original_doc_filename_for_chunking_context\", \"this document\")\n",
        "    block_type = block_metadata.get(\"type\", \"text_paragraph\")\n",
        "    page_num = block_metadata.get(\"page_number\", \"N/A\")\n",
        "    doc_type = block_metadata.get(\"document_type\", \"document\") # Crucial: Get original document type\n",
        "    font_size = block_metadata.get(\"font_size\", 0) # Only relevant for PDFs originally\n",
        "    is_bold_str = \" (Bold)\" if block_metadata.get(\"is_bold\", False) else \"\" # Only relevant for PDFs originally\n",
        "    source_img_filename = block_metadata.get(\"source_image_filename\")\n",
        "    ocr_source = block_metadata.get(\"ocr_source\")\n",
        "\n",
        "    prefix = \"\"\n",
        "\n",
        "    # General document type indicator for clarity in context\n",
        "    doc_type_label = {\n",
        "        \"pdf\": \"PDF document\",\n",
        "        \"docx\": \"Word document\",\n",
        "        \"xlsx\": \"Excel document\",\n",
        "        \"pptx\": \"PowerPoint presentation\",\n",
        "        \"image\": \"Image file\"\n",
        "    }.get(doc_type, \"document\")\n",
        "\n",
        "    # Specific prefixes for different block types\n",
        "    if block_type == \"table_markdown\":\n",
        "        prefix = f\"Context: Data table from {doc_type_label} '{orig_filename}', section/page {page_num}.\\nContent:\\n\"\n",
        "    elif block_type == \"image_ocr_text\":\n",
        "        img_info = f\"image '{source_img_filename}'\" if source_img_filename else \"an image\"\n",
        "        prefix = f\"Context: OCR transcription from {img_info} on page {page_num} of {doc_type_label} '{orig_filename}'.\\nContent:\\n\"\n",
        "    elif block_type == \"image_description\":\n",
        "        img_info = f\"image '{source_img_filename}'\" if source_img_filename else \"an image\"\n",
        "        prefix = f\"Context: Description of {img_info} on page {page_num} of {doc_type_label} '{orig_filename}'.\\nContent:\\n\"\n",
        "    elif block_type == \"full_page_ocr_text\":\n",
        "        ocr_source_info = f\"via {ocr_source}\" if ocr_source else \"\"\n",
        "        prefix = f\"Context: Full page OCR text from scanned page {page_num} of {doc_type_label} '{orig_filename}' {ocr_source_info}.\\nContent:\\n\"\n",
        "    elif block_type == \"title_document\":\n",
        "        prefix = f\"Context: Document Title from {doc_type_label} '{orig_filename}'(P{page_num},F{font_size:.0f}{is_bold_str}): \\n\"\n",
        "    elif block_type in [\"h1_heading\", \"h2_heading\", \"h3_heading\", \"heading_1\", \"heading_2\", \"heading_3\", \"h_candidate_uppercase\", \"pptx_slide_text\"]:\n",
        "        level_map = {\n",
        "            \"h1_heading\": \"H1\", \"h2_heading\": \"H2\", \"h3_heading\": \"H3\",\n",
        "            \"heading_1\": \"Heading 1\", \"heading_2\": \"Heading 2\", \"heading_3\": \"Heading 3\",\n",
        "            \"h_candidate_uppercase\": \"Potential Heading\",\n",
        "            \"pptx_slide_text\": \"PowerPoint Slide Content\" # Special case for PPTX converted content\n",
        "        }\n",
        "        level_str = level_map.get(block_type, \"Heading\")\n",
        "        prefix = f\"Context: {level_str} from {doc_type_label} '{orig_filename}', section/page {page_num} (F{font_size:.0f}{is_bold_str}).\\nContent:\\n\"\n",
        "    elif block_type == \"excel_sheet_data\": # For sheets that might have been detected as whole text blocks in PDF\n",
        "        prefix = f\"Context: Data from Excel sheet of {doc_type_label} '{orig_filename}', sheet/page {page_num}.\\nContent:\\n\"\n",
        "    elif block_type == \"docx_paragraph\": # For paragraphs from Word docs\n",
        "        prefix = f\"Context: Paragraph from Word document '{orig_filename}', page {page_num}.\\nContent:\\n\"\n",
        "    else: # Default for generic text paragraphs\n",
        "        # Heuristic for detecting potential sub-headings within generic text blocks\n",
        "        lines = original_chunk_text.split('\\n', 1)\n",
        "        first_line = lines[0].strip()\n",
        "        # Check if first line is short, few words, not ending in punctuation (common for headings)\n",
        "        if (1 < len(first_line.split()) < 9 and\n",
        "            count_tokens(first_line) < 40 and\n",
        "            not first_line.endswith(('.', '?', '!')) and\n",
        "            (first_line.isupper() or first_line.istitle() or is_bold_str)):\n",
        "            prefix = f\"Context: From {doc_type_label} '{orig_filename}', section/page {page_num}, likely section titled '{first_line}'.\\nContent:\\n\"\n",
        "        else:\n",
        "            prefix = f\"Context: Text from {doc_type_label} '{orig_filename}', section/page {page_num}.\\nContent:\\n\"\n",
        "\n",
        "    return prefix + original_chunk_text\n",
        "\n",
        "def _chunk_parsed_blocks(\n",
        "    parsed_blocks: List[Dict], collection_name_as_doc_id: str, original_filename_for_meta: str\n",
        ") -> List[Dict]:\n",
        "    all_final_chunks = []\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=PRIMARY_CHUNK_SIZE_TOKENS,\n",
        "        chunk_overlap=PRIMARY_CHUNK_OVERLAP_TOKENS,\n",
        "        length_function=count_tokens,\n",
        "        separators=SEPARATORS_HIERARCHICAL,\n",
        "        keep_separator=False,\n",
        "    )\n",
        "    for block_idx, block in enumerate(parsed_blocks):\n",
        "        original_content = block.get(\"content\", \"\").strip()\n",
        "        block_type = block.get(\"type\", \"unknown\")\n",
        "        if not original_content or \"noise\" in block_type or block_type == \"error\":\n",
        "            continue\n",
        "\n",
        "        current_block_metadata = {k:v for k,v in block.items() if k != \"content\"}\n",
        "        current_block_metadata.update({\"doc_id\": collection_name_as_doc_id, \"original_filename\": original_filename_for_meta})\n",
        "        current_block_metadata.pop(\"content\", None)\n",
        "\n",
        "        APP_STATE[\"original_doc_filename_for_chunking_context\"] = original_filename_for_meta\n",
        "        temp_prefixed_content = _prepend_context_to_chunk(original_content, block)\n",
        "        APP_STATE[\"original_doc_filename_for_chunking_context\"] = None\n",
        "\n",
        "        prefixed_token_count = count_tokens(temp_prefixed_content)\n",
        "        original_token_count = count_tokens(original_content)\n",
        "\n",
        "        should_split = False\n",
        "        if prefixed_token_count > EFFECTIVE_MAX_CHUNK_TOKENS:\n",
        "            logger.warning(f\"Block {block.get('block_id', f'b{block_idx}')} (pg:{block.get('page_number')}, type:{block_type}) prefixed form ({prefixed_token_count} tokens) exceeds max {EFFECTIVE_MAX_CHUNK_TOKENS}. Will be split.\")\n",
        "            should_split = True\n",
        "        elif block_type in [\"text_paragraph\", \"full_page_ocr_text\", \"docx_paragraph\", \"excel_sheet_data\", \"pptx_slide_text\", \"image_ocr_text\", \"image_description\", \"table_markdown\"] and original_token_count > PRIMARY_CHUNK_SIZE_TOKENS: # Added table_markdown to types that can be split if too large\n",
        "            should_split = True\n",
        "        elif block_type not in [\"text_paragraph\", \"full_page_ocr_text\", \"docx_paragraph\", \"excel_sheet_data\", \"pptx_slide_text\", \"image_ocr_text\", \"image_description\", \"table_markdown\"] and original_token_count > ATOMIC_BLOCK_NO_SPLIT_THRESHOLD:\n",
        "             pass\n",
        "\n",
        "        if not should_split:\n",
        "            chunk_id = f\"{block.get('block_id', f'b{block_idx}')}_chunk0\"\n",
        "            final_meta = {**current_block_metadata, \"chunk_id\": chunk_id, \"original_block_id\": block.get('block_id')}\n",
        "            all_final_chunks.append({\n",
        "                \"text_content_for_embedding\": temp_prefixed_content,\n",
        "                \"original_chunk_text\": original_content,\n",
        "                \"metadata\": final_meta\n",
        "            })\n",
        "        else:\n",
        "            sub_texts = text_splitter.split_text(original_content)\n",
        "            logger.debug(f\"  Splitting block {block.get('block_id')} ({block_type}, pg {block.get('page_number')}) into {len(sub_texts)} sub-chunks.\")\n",
        "            for i, sub_text_content in enumerate(sub_texts):\n",
        "                sub_text_content = sub_text_content.strip()\n",
        "                if not sub_text_content: continue\n",
        "\n",
        "                APP_STATE[\"original_doc_filename_for_chunking_context\"] = original_filename_for_meta\n",
        "                final_sub_chunk_for_embedding = _prepend_context_to_chunk(sub_text_content, block)\n",
        "                APP_STATE[\"original_doc_filename_for_chunking_context\"] = None\n",
        "\n",
        "                final_sub_chunk_token_count = count_tokens(final_sub_chunk_for_embedding)\n",
        "                if final_sub_chunk_token_count > EFFECTIVE_MAX_CHUNK_TOKENS:\n",
        "                    logger.warning(f\"    Sub-chunk {i} of {block.get('block_id')} (pg:{block.get('page_number')}) still too large ({final_sub_chunk_token_count} tokens) after splitting. TRUNCATING. Text: '{final_sub_chunk_for_embedding[:100]}...'\")\n",
        "                    tokenizer = APP_STATE.get(\"tokenizer\")\n",
        "                    if tokenizer:\n",
        "                        encoded = tokenizer.encode(final_sub_chunk_for_embedding, disallowed_special=())\n",
        "                        final_sub_chunk_for_embedding = tokenizer.decode(encoded[:EFFECTIVE_MAX_CHUNK_TOKENS])\n",
        "                    else:\n",
        "                        final_sub_chunk_for_embedding = final_sub_chunk_for_embedding[:EFFECTIVE_MAX_CHUNK_TOKENS * 4]\n",
        "\n",
        "                chunk_id = f\"{block.get('block_id', f'b{block_idx}')}_subchunk{i}\"\n",
        "                final_meta = {**current_block_metadata, \"chunk_id\": chunk_id, \"original_block_id\": block.get('block_id'), \"split_index\": i}\n",
        "                all_final_chunks.append({\n",
        "                    \"text_content_for_embedding\": final_sub_chunk_for_embedding,\n",
        "                    \"original_chunk_text\": sub_text_content,\n",
        "                    \"metadata\": final_meta\n",
        "                })\n",
        "    logger.info(f\"Chunking for '{original_filename_for_meta}': {len(all_final_chunks)} final chunks.\")\n",
        "    return all_final_chunks\n",
        "\n",
        "def _embed_chunks(chunks_to_embed: List[Dict], batch_size: int = 16) -> Optional[List[Dict]]:\n",
        "    if not APP_STATE.get(\"rag_models_loaded\") or not APP_STATE.get(\"azure_embedding_client\"):\n",
        "        logger.error(\"Azure Embedding client not ready.\")\n",
        "        return None\n",
        "\n",
        "    texts = [c[\"text_content_for_embedding\"] for c in chunks_to_embed]\n",
        "    if not texts:\n",
        "        logger.warning(\"No texts to embed.\")\n",
        "        return []\n",
        "\n",
        "    logger.info(f\"Embedding {len(texts)} chunks using Azure OpenAI '{AZURE_EMBEDDING_MODEL_NAME}'...\")\n",
        "    try:\n",
        "        response = APP_STATE[\"azure_embedding_client\"].embed(\n",
        "            input=texts,\n",
        "            model=AZURE_EMBEDDING_MODEL_NAME\n",
        "        )\n",
        "        embeddings_list = []\n",
        "        for item in response.data:\n",
        "            embeddings_list.append(item.embedding)\n",
        "\n",
        "        embeddings_np = np.array(embeddings_list, dtype=np.float32)\n",
        "\n",
        "        for c, emb in zip(chunks_to_embed, embeddings_np):\n",
        "            c[\"embedding_vector\"] = emb.tolist()\n",
        "        logger.success(f\"Embedded {len(texts)} chunks using Azure OpenAI.\")\n",
        "        return chunks_to_embed\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Azure Embedding FAILED: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _store_chunks_in_chromadb(\n",
        "    chunks_with_embeddings: List[Dict], collection: chromadb.api.models.Collection.Collection\n",
        ") -> int:\n",
        "    if not collection:\n",
        "        logger.error(\"Chroma collection invalid.\")\n",
        "        return 0\n",
        "\n",
        "    ids, embs, metas, docs = [], [], [], []\n",
        "    valid_count = 0\n",
        "    for chunk in chunks_with_embeddings:\n",
        "        if \"embedding_vector\" not in chunk or not chunk.get(\"metadata\", {}).get(\"chunk_id\"):\n",
        "            logger.warning(f\"Skip chunk: missing embedding_vector/chunk_id. BlockID: {chunk.get('metadata',{}).get('original_block_id')}\")\n",
        "            continue\n",
        "\n",
        "        meta = chunk[\"metadata\"]\n",
        "        ids.append(meta[\"chunk_id\"])\n",
        "        embs.append(chunk[\"embedding_vector\"])\n",
        "        docs.append(chunk[\"original_chunk_text\"])\n",
        "\n",
        "        clean_meta = {}\n",
        "        for k, v in meta.items():\n",
        "            if k == \"bbox\" and isinstance(v, (list, tuple)):\n",
        "                try: clean_meta[k] = json.dumps(v)\n",
        "                except TypeError: logger.warning(f\"Could not serialize bbox {v}. Storing as str.\"); clean_meta[k] = str(v)\n",
        "            elif isinstance(v, (str, int, float, bool)) or v is None:\n",
        "                clean_meta[k] = v\n",
        "            else:\n",
        "                clean_meta[k] = str(v)\n",
        "        metas.append(clean_meta)\n",
        "        valid_count += 1\n",
        "\n",
        "    if not ids:\n",
        "        logger.warning(f\"No valid chunks to store in '{collection.name}'.\")\n",
        "        return 0\n",
        "    try:\n",
        "        collection.upsert(ids=ids, embeddings=embs, metadatas=metas, documents=docs)\n",
        "        logger.success(f\"Stored {valid_count} chunks. Collection '{collection.name}' total: {collection.count()}.\")\n",
        "        return valid_count\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Chroma upsert FAILED for '{collection.name}': {e}\", exc_info=True)\n",
        "        return 0\n",
        "\n",
        "def ingest_document_into_knowledge_base(\n",
        "    parsed_blocks: List[Dict], collection_name: str, original_filename: str,\n",
        "    target_collection: chromadb.api.models.Collection.Collection\n",
        ") -> bool:\n",
        "    start_time = time.time()\n",
        "    logger.info(f\"Ingesting '{original_filename}' into collection: {collection_name}\")\n",
        "\n",
        "    if not APP_STATE.get(\"rag_models_loaded\") or not target_collection:\n",
        "        logger.error(\n",
        "            f\"Ingest prereqs FAIL. RAG loaded: {APP_STATE.get('rag_models_loaded')}, \"\n",
        "            f\"Collection valid: {target_collection is not None}.\"\n",
        "        )\n",
        "        return False\n",
        "\n",
        "    chunks_for_embedding = _chunk_parsed_blocks(parsed_blocks, collection_name, original_filename)\n",
        "    if not chunks_for_embedding:\n",
        "        logger.error(f\"Chunking for '{original_filename}' FAIL.\")\n",
        "        return False\n",
        "\n",
        "    chunks_with_vectors = _embed_chunks(chunks_for_embedding)\n",
        "    if not chunks_with_vectors:\n",
        "        logger.error(f\"Embedding for '{original_filename}' FAIL.\")\n",
        "        return False\n",
        "\n",
        "    num_stored = _store_chunks_in_chromadb(chunks_with_vectors, target_collection)\n",
        "    success = num_stored > 0\n",
        "\n",
        "    logger.log(\n",
        "        \"SUCCESS\" if success else \"ERROR\",\n",
        "        f\"Ingest '{original_filename}' ({collection_name}) in {time.time()-start_time:.2f}s. \"\n",
        "        f\"Stored: {num_stored}.\"\n",
        "    )\n",
        "    return success\n",
        "\n",
        "# --- 10. Retrieval Logic ---\n",
        "def retrieve_relevant_context(\n",
        "    query_text: str, target_collection_names: List[str],\n",
        "    k_initial_retrieval: int = 20, k_final_target: int = 12,\n",
        "    use_reranking: bool = True\n",
        ") -> Dict:\n",
        "    retrieval_start_time = time.time()\n",
        "    payload = {\n",
        "        \"query\": query_text,\n",
        "        \"formatted_context_for_llm\": \"Error: Retrieval fail.\",\n",
        "        \"source_chunks_retrieved\": [],\n",
        "        \"retrieval_metadata\": {\n",
        "            \"collections_queried\": target_collection_names,\n",
        "            \"k_initial\": k_initial_retrieval,\n",
        "            \"k_final\": k_final_target,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    required_keys = [\"rag_models_loaded\", \"db_initialized\", \"chroma_client\", \"azure_embedding_client\"]\n",
        "    if not all(APP_STATE.get(k) for k in required_keys):\n",
        "        payload[\"retrieval_metadata\"][\"error\"] = \"RAG/DB systems not ready (Azure Embedding client missing).\"\n",
        "        logger.error(payload[\"retrieval_metadata\"][\"error\"])\n",
        "        return payload\n",
        "\n",
        "    try:\n",
        "        query_response = APP_STATE[\"azure_embedding_client\"].embed(\n",
        "            input=[query_text],\n",
        "            model=AZURE_EMBEDDING_MODEL_NAME\n",
        "        )\n",
        "        query_emb = query_response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        payload[\"retrieval_metadata\"][\"error\"] = f\"Query embed fail with Azure: {e}\"\n",
        "        logger.error(f\"Query embed fail with Azure: {e}\")\n",
        "        return payload\n",
        "\n",
        "    candidates = []\n",
        "    noise_types = [\"noise_header_footer\", \"noise_short_irrelevant\", \"info\"]\n",
        "    is_about_query = any(p in query_text.lower() for p in [\"about this document\", \"summarize\", \"overview\", \"main idea\", \"tell me about\"])\n",
        "    anchors = []\n",
        "\n",
        "    for coll_name in target_collection_names:\n",
        "        coll = get_or_create_collection_cached(coll_name)\n",
        "        if not coll:\n",
        "            logger.warning(f\"Skip collection '{coll_name}' as it's invalid or couldn't be accessed.\")\n",
        "            continue\n",
        "\n",
        "        current_collection_candidates_map = {}\n",
        "\n",
        "        try:\n",
        "            res = coll.query(\n",
        "                query_embeddings=[query_emb],\n",
        "                n_results=k_initial_retrieval,\n",
        "                where={\"type\": {\"$nin\": noise_types}},\n",
        "                include=['metadatas', 'documents', 'distances']\n",
        "            )\n",
        "            if res and res.get('ids') and res['ids'][0]:\n",
        "                for i in range(len(res['ids'][0])):\n",
        "                    candidate_id = res['ids'][0][i]\n",
        "                    current_collection_candidates_map[candidate_id] = {\n",
        "                        \"id\": candidate_id,\n",
        "                        \"text_content\": res['documents'][0][i],\n",
        "                        \"metadata\": res['metadatas'][0][i],\n",
        "                        \"retrieval_score_distance\": res['distances'][0][i],\n",
        "                        \"source_stage\": f\"S1_Semantic_{coll_name}\"\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Chroma S1 semantic query failed for '{coll_name}': {e}\")\n",
        "\n",
        "        if is_about_query and len(target_collection_names) == 1:\n",
        "            try:\n",
        "                res_anc = coll.query(\n",
        "                    query_embeddings=[query_emb],\n",
        "                    n_results=5,\n",
        "                    where={\"$and\": [\n",
        "                        {\"type\": {\"$in\": [\"title_document\", \"h1_heading\", \"pptx_slide_text\", \"heading_1\", \"excel_sheet_data\", \"image_ocr_text\", \"image_description\"]}},\n",
        "                        {\"page_number\": {\"$lte\": 2}}\n",
        "                    ]},\n",
        "                    include=['metadatas', 'documents', 'distances']\n",
        "                )\n",
        "                if res_anc and res_anc.get('ids') and res_anc['ids'][0]:\n",
        "                    page_num_key = lambda x: x[\"metadata\"].get(\"page_number\", 999)\n",
        "                    temp_anchors_from_coll = sorted([\n",
        "                        {\n",
        "                            \"id\": res_anc['ids'][0][i],\n",
        "                            \"text_content\": res_anc['documents'][0][i],\n",
        "                            \"metadata\": res_anc['metadatas'][0][i],\n",
        "                            \"retrieval_score_distance\": res_anc['distances'][0][i],\n",
        "                            \"source_stage\": f\"S2_Anchor_{coll_name}\"\n",
        "                        } for i in range(len(res_anc['ids'][0]))\n",
        "                    ], key=lambda x: (page_num_key(x), x[\"retrieval_score_distance\"]))\n",
        "\n",
        "                    for anchor_cand in temp_anchors_from_coll[:min(2, len(temp_anchors_from_coll))]:\n",
        "                        current_collection_candidates_map[anchor_cand['id']] = anchor_cand\n",
        "                        anchors.append(anchor_cand)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Chroma S2 anchor query failed for '{coll_name}': {e}\")\n",
        "\n",
        "        candidates.extend(list(current_collection_candidates_map.values()))\n",
        "\n",
        "    final_anchors = list({ac['id']: ac for ac in anchors}.values())\n",
        "\n",
        "    if not candidates:\n",
        "        payload[\"formatted_context_for_llm\"] = \"No relevant context found in the document(s).\"\n",
        "        return payload\n",
        "\n",
        "    selected_chunks = list(final_anchors)\n",
        "    general_candidates = [c for c in candidates if c['id'] not in {fa['id'] for fa in final_anchors}]\n",
        "\n",
        "    slots_left_for_general = k_final_target - len(selected_chunks)\n",
        "\n",
        "    if slots_left_for_general > 0 and general_candidates:\n",
        "        if use_reranking and APP_STATE.get(\"reranker_model\"):\n",
        "            try:\n",
        "                rerank_pairs = [[query_text, chunk['text_content']] for chunk in general_candidates]\n",
        "\n",
        "                scores = APP_STATE[\"reranker_model\"].predict(rerank_pairs, show_progress_bar=False)\n",
        "                for chunk, score in zip(general_candidates, scores):\n",
        "                    chunk['rerank_score'] = score\n",
        "\n",
        "                if is_about_query and len(target_collection_names) == 1:\n",
        "                    for chunk in general_candidates:\n",
        "                        boost = 0.0\n",
        "                        meta_type = chunk.get(\"metadata\",{}).get(\"type\")\n",
        "                        meta_page = chunk.get(\"metadata\",{}).get(\"page_number\", 999)\n",
        "                        if meta_type == \"title_document\": boost = 5.0\n",
        "                        elif meta_type in [\"h1_heading\", \"heading_1\", \"pptx_slide_text\"] and meta_page <= 2: boost = 2.5\n",
        "                        elif meta_type in [\"h2_heading\", \"heading_2\"] and meta_page <= 3: boost = 1.0\n",
        "                        elif meta_type in [\"excel_sheet_data\", \"image_ocr_text\", \"image_description\"] and meta_page <= 2: boost = 1.5\n",
        "                        chunk['rerank_score'] = chunk.get('rerank_score', 0) + boost\n",
        "\n",
        "                general_candidates.sort(key=lambda x: x.get('rerank_score', -float('inf')), reverse=True)\n",
        "                payload[\"retrieval_metadata\"][\"reranked_general_candidates\"] = True\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Reranking FAILED: {e}. Falling back to semantic scores for general candidates.\")\n",
        "                general_candidates.sort(key=lambda x: x.get('retrieval_score_distance', float('inf')))\n",
        "                payload[\"retrieval_metadata\"][\"reranked_general_candidates\"] = False\n",
        "        else:\n",
        "            general_candidates.sort(key=lambda x: x.get('retrieval_score_distance', float('inf')))\n",
        "            payload[\"retrieval_metadata\"][\"reranked_general_candidates\"] = False\n",
        "\n",
        "        selected_chunks.extend(general_candidates[:slots_left_for_general])\n",
        "\n",
        "    if not selected_chunks:\n",
        "        payload[\"formatted_context_for_llm\"] = \"No relevant context found after selection and reranking.\"\n",
        "        return payload\n",
        "\n",
        "    def sort_key_final(chunk):\n",
        "        score = chunk.get('rerank_score', -float('inf')) if use_reranking and APP_STATE.get(\"reranker_model\") else -chunk.get('retrieval_score_distance', float('-inf'))\n",
        "        page = chunk.get('metadata', {}).get('page_number', 999)\n",
        "        bbox_str = chunk.get('metadata', {}).get('bbox', None)\n",
        "        y_order = float('inf')\n",
        "        if bbox_str:\n",
        "            try:\n",
        "                bbox = json.loads(bbox_str) if isinstance(bbox_str, str) else bbox_str\n",
        "                if isinstance(bbox, (list, tuple)) and len(bbox) >= 2:\n",
        "                    y_order = bbox[1]\n",
        "            except:\n",
        "                pass\n",
        "        return (score, -page, -y_order)\n",
        "\n",
        "    selected_chunks.sort(key=sort_key_final, reverse=True)\n",
        "\n",
        "    formatted_context_parts = []\n",
        "    for i, chunk in enumerate(selected_chunks):\n",
        "        meta = chunk['metadata']\n",
        "        orig_fname = meta.get('original_filename', 'UnknownDocument')\n",
        "        doc_type = meta.get('document_type', 'document')\n",
        "        bbox_val = meta.get(\"bbox\", \"N/A\")\n",
        "\n",
        "        content_for_llm = chunk.get('text_content', '')\n",
        "\n",
        "        bbox_str_fmt = \"N/A\"\n",
        "        if isinstance(bbox_val, str):\n",
        "            try:\n",
        "                parsed_bbox = json.loads(bbox_val)\n",
        "                bbox_str_fmt = f\"({parsed_bbox[0]:.0f},{parsed_bbox[1]:.0f},{parsed_bbox[2]:.0f},{parsed_bbox[3]:.0f})\"\n",
        "            except (json.JSONDecodeError, TypeError, IndexError):\n",
        "                bbox_str_fmt = bbox_val\n",
        "        elif isinstance(bbox_val, (list, tuple)) and len(bbox_val) == 4:\n",
        "            bbox_str_fmt = f\"({bbox_val[0]:.0f},{bbox_val[1]:.0f},{bbox_val[2]:.0f},{bbox_val[3]:.0f})\"\n",
        "\n",
        "\n",
        "        header = (\n",
        "            f\"--- Context Source {i+1} (Doc:'{orig_fname}', Type:{doc_type}, SrcStage:{chunk.get('source_stage','N/A')}) ---\\n\"\n",
        "            f\"Page/Section: {meta.get('page_number','?')}, Type: {meta.get('type','?')}, ChunkID: {meta.get('chunk_id','N/A')}\\n\"\n",
        "            f\"Font Size: {meta.get('font_size',0):.0f}, Bold: {meta.get('is_bold',False)}, Semantic Heading: {meta.get('is_semantic_heading',False)}\\n\"\n",
        "            f\"BBox: {bbox_str_fmt}, Distance: {chunk.get('retrieval_score_distance',-1):.4f}\"\n",
        "        )\n",
        "        if 'rerank_score' in chunk:\n",
        "            header += f\", RerankScore: {chunk['rerank_score']:.4f}\"\n",
        "\n",
        "        formatted_context_parts.append(f\"{header}\\nContent:\\n{content_for_llm}\\n--- End Src {i+1} ---\")\n",
        "        payload[\"source_chunks_retrieved\"].append(chunk)\n",
        "\n",
        "    payload[\"formatted_context_for_llm\"] = \"\\n\\n\".join(formatted_context_parts)\n",
        "    payload[\"retrieval_metadata\"][\"final_chunk_count\"] = len(selected_chunks)\n",
        "\n",
        "    logger.success(\n",
        "        f\"Context retrieval for '{query_text[:30]}...' in {time.time()-retrieval_start_time:.2f}s. \"\n",
        "        f\"Final chunks: {len(selected_chunks)}.\"\n",
        "    )\n",
        "    return payload\n",
        "\n",
        "# --- 11. Response Generation ---\n",
        "def _generate_llm_messages(\n",
        "    user_query: str,\n",
        "    context_str: str,\n",
        "    document_identifier: str,\n",
        "    dynamic_role_instructions: Optional[str],\n",
        "    is_summary_context: bool,\n",
        ") -> List[Dict]:\n",
        "    system_content_parts = [\n",
        "        f\"You are {AI_PERSONA_NAME}, {AI_ROLE_DESCRIPTION}\",\n",
        "        AI_CORE_DIRECTIVE.replace(\"'{document_name}'\", f\"'{document_identifier}'\"),\n",
        "        AI_ANSWERING_STYLE_REFINED,\n",
        "        AI_CONTEXTUAL_PRIORITIZATION_POLICY,\n",
        "        AI_CITATION_POLICY_TEXT,\n",
        "        AI_NO_ANSWER_POLICY,\n",
        "    ]\n",
        "\n",
        "    if is_summary_context:\n",
        "        system_content_parts.append(AI_SUMMARIZATION_POLICY)\n",
        "        context_header = \"PROVIDED SUMMARY (Answer ONLY from this summary):\"\n",
        "    else:\n",
        "        context_header = \"PROVIDED CONTEXT (Answer ONLY from this):\"\n",
        "\n",
        "    if dynamic_role_instructions and dynamic_role_instructions.strip():\n",
        "        system_content_parts.extend([\n",
        "            \"\\n--- ADDITIONAL ROLE GUIDANCE ---\",\n",
        "            dynamic_role_instructions.strip(),\n",
        "            \"--- END GUIDANCE ---\",\n",
        "            \"Strictly adhere to ALL guidance.\"\n",
        "        ])\n",
        "\n",
        "    system_message = {\"role\": \"system\", \"content\": \"\\n\\n\".join(system_content_parts).strip()}\n",
        "    user_message = {\"role\": \"user\", \"content\": f\"USER'S QUERY: {user_query.strip()}\\n\\n{context_header}\\n{context_str.strip()}\"}\n",
        "\n",
        "    return [system_message, user_message]\n",
        "\n",
        "\n",
        "def generate_standard_llm_response(\n",
        "    user_query: str, formatted_context_from_rag: str,\n",
        "    source_chunks_from_rag: List[Dict], doc_name_identifier: str,\n",
        "    dynamic_role_info: Optional[str], response_payload: Dict\n",
        ") -> Dict:\n",
        "    messages = _generate_llm_messages(\n",
        "        user_query, formatted_context_from_rag, doc_name_identifier,\n",
        "        dynamic_role_info, is_summary_context=False\n",
        "    )\n",
        "    response_payload[\"llm_prompt_sent\"] = messages\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Sending RAG request to OpenAI ('{GENERATION_MODEL_NAME}'). Query: '{user_query[:70]}...'\")\n",
        "        gen_start_time = time.time()\n",
        "\n",
        "        if not APP_STATE.get(\"azure_llm_client\"):\n",
        "            logger.error(\"OpenAI LLM client instance not available.\")\n",
        "            response_payload.update({\"answer_text\": \"Error: AI model not configured.\", \"error_message\": \"AI model not configured.\"})\n",
        "            return response_payload\n",
        "\n",
        "        openai_api_response = APP_STATE[\"azure_llm_client\"].chat.completions.create(\n",
        "            model=AZURE_LLM_MODEL_NAME,\n",
        "            messages=messages,\n",
        "            temperature=0.25,\n",
        "            max_tokens=1000,\n",
        "            top_p=1.0,\n",
        "        )\n",
        "        response_payload[\"generation_time_s\"] = round(time.time() - gen_start_time, 2)\n",
        "        logger.success(f\"OpenAI LLM response in {response_payload['generation_time_s']:.2f}s.\")\n",
        "\n",
        "        try:\n",
        "            response_payload[\"llm_full_response_obj_str\"] = str(openai_api_response)\n",
        "        except Exception:\n",
        "            response_payload[\"llm_full_response_obj_str\"] = \"Could not serialize full OpenAI response object.\"\n",
        "\n",
        "        if openai_api_response.choices and openai_api_response.choices[0].message:\n",
        "            generated_text = openai_api_response.choices[0].message.content.strip()\n",
        "            response_payload[\"answer_text\"] = generated_text\n",
        "\n",
        "            citations_found_raw = re.findall(r'(\\[Doc:[^\\]]+?CkID:[^\\]]+?\\])', generated_text)\n",
        "            if citations_found_raw:\n",
        "                logger.warning(\n",
        "                    f\"LLM included {len(citations_found_raw)} bracketed citations despite instruction. \"\n",
        "                    \"These should not be displayed in the final answer if the prompt was followed.\"\n",
        "                )\n",
        "            response_payload[\"parsed_citations\"] = [{\"tag\": c} for c in citations_found_raw]\n",
        "        else:\n",
        "            logger.error(\"OpenAI API response did not contain expected message content.\")\n",
        "            response_payload.update({\n",
        "                \"answer_text\": \"Error: Could not parse text from AI response.\",\n",
        "                \"error_message\": \"OpenAI response parsing error: No choices or message found.\"\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"General error during OpenAI API call: {e}\", exc_info=True)\n",
        "        response_payload.update({\n",
        "            \"answer_text\": f\"Error: LLM call failed ({type(e).__name__}).\",\n",
        "            \"error_message\": str(e)\n",
        "        })\n",
        "\n",
        "    response_payload.setdefault(\"answer_text\", \"Error: Processing failed to produce an answer.\")\n",
        "    response_payload.setdefault(\"parsed_citations\", [])\n",
        "    return response_payload\n",
        "\n",
        "def generate_llm_response(\n",
        "    user_query: str, formatted_context_from_rag: str,\n",
        "    source_chunks_from_rag: List[Dict], default_doc_name: str = \"the document\",\n",
        "    dynamic_role_info: Optional[str] = None, is_aboutness_query_flag: bool = False\n",
        ") -> Dict:\n",
        "    response_payload = {\n",
        "        \"answer_text\": \"Error: LLM response could not be generated.\",\n",
        "        \"llm_prompt_sent\": \"\",\n",
        "        \"llm_full_response_obj_str\": None,\n",
        "        \"parsed_citations\": [],\n",
        "        \"error_message\": None,\n",
        "        \"generation_time_s\": 0.0,\n",
        "        \"summarization_chain_used\": False,\n",
        "    }\n",
        "\n",
        "    required_llm_keys = [\"llm_initialized\", \"azure_llm_client\", \"langchain_llm\"]\n",
        "    if not all(APP_STATE.get(k) for k in required_llm_keys):\n",
        "        logger.error(\"LLM systems not initialized (OpenAI client or LangChain LLM missing).\")\n",
        "        response_payload[\"error_message\"] = \"LLM systems not initialized.\"\n",
        "        return response_payload\n",
        "\n",
        "    doc_names = sorted(list(set(\n",
        "        c['metadata'].get('original_filename', default_doc_name)\n",
        "        for c in source_chunks_from_rag if c.get('metadata')\n",
        "    )))\n",
        "    doc_id_for_prompt = default_doc_name\n",
        "    if doc_names:\n",
        "        if len(doc_names) == 1:\n",
        "            doc_id_for_prompt = f\"document '{doc_names[0]}'\"\n",
        "        else:\n",
        "            doc_id_for_prompt = f\"documents ({', '.join(doc_names)})\"\n",
        "\n",
        "    context_total_text = \"\\n\\n\".join([\n",
        "        c.get(\"original_chunk_text\", c.get(\"text_content\",\"\"))\n",
        "        for c in source_chunks_from_rag\n",
        "    ])\n",
        "    context_token_count = count_tokens(context_total_text)\n",
        "\n",
        "    if is_aboutness_query_flag and context_token_count > MAX_TOKENS_FOR_DIRECT_LLM_SUMMARIZATION and source_chunks_from_rag:\n",
        "        logger.info(\n",
        "            f\"Aboutness query with large context ({context_token_count} tokens > {MAX_TOKENS_FOR_DIRECT_LLM_SUMMARIZATION}). \"\n",
        "            \"Attempting LangChain map-reduce summarization.\"\n",
        "        )\n",
        "        response_payload[\"summarization_chain_used\"] = True\n",
        "\n",
        "        langchain_documents = [\n",
        "            Document(page_content=chunk.get(\"original_chunk_text\", chunk.get(\"text_content\",\"\")), metadata=chunk.get(\"metadata\", {}))\n",
        "            for chunk in source_chunks_from_rag if chunk.get(\"original_chunk_text\", chunk.get(\"text_content\",\"\")).strip()\n",
        "        ]\n",
        "\n",
        "        if not langchain_documents:\n",
        "            logger.warning(\"No valid documents for LangChain summarization. Falling back to standard generation.\")\n",
        "            return generate_standard_llm_response(\n",
        "                user_query, formatted_context_from_rag, source_chunks_from_rag,\n",
        "                doc_id_for_prompt, dynamic_role_info, response_payload\n",
        "            )\n",
        "        try:\n",
        "            summarize_chain = load_summarize_chain(\n",
        "                llm=APP_STATE[\"langchain_llm\"], chain_type=\"map_reduce\", verbose=False\n",
        "            )\n",
        "            summarization_question = (\n",
        "                f\"Provide a comprehensive summary of the key information in '{doc_id_for_prompt}' \"\n",
        "                f\"that is relevant to the user's query: '{user_query}'\"\n",
        "            )\n",
        "\n",
        "            start_time_lc_summarize = time.time()\n",
        "            summary_result = summarize_chain.invoke({\n",
        "                \"input_documents\": langchain_documents,\n",
        "                \"question\": summarization_question\n",
        "            })\n",
        "            response_payload[\"generation_time_s\"] = round(time.time() - start_time_lc_summarize, 2)\n",
        "\n",
        "            raw_summary_text = summary_result.get(\"output_text\", \"Error: Summarization failed to produce text.\").strip()\n",
        "            logger.success(\n",
        "                f\"LangChain summarization completed in {response_payload['generation_time_s']:.2f}s. \"\n",
        "                f\"Raw summary length: {len(raw_summary_text)} characters.\"\n",
        "            )\n",
        "\n",
        "            if not raw_summary_text or \"Error:\" in raw_summary_text or \"failed to produce\" in raw_summary_text.lower():\n",
        "                logger.warning(\"LangChain summary was empty or indicated failure. Falling back to standard generation.\")\n",
        "                return generate_standard_llm_response(\n",
        "                    user_query, formatted_context_from_rag, source_chunks_from_rag,\n",
        "                    doc_id_for_prompt, dynamic_role_info, response_payload\n",
        "                )\n",
        "\n",
        "            messages_refine = _generate_llm_messages(\n",
        "                user_query, raw_summary_text, doc_id_for_prompt,\n",
        "                dynamic_role_info, is_summary_context=True\n",
        "            )\n",
        "            response_payload[\"llm_prompt_sent\"] = messages_refine\n",
        "\n",
        "            start_time_final_refine = time.time()\n",
        "            final_openai_response = APP_STATE[\"azure_llm_client\"].chat.completions.create(\n",
        "                model=AZURE_LLM_MODEL_NAME,\n",
        "                messages=messages_refine,\n",
        "                temperature=0.25,\n",
        "                max_tokens=1000,\n",
        "                top_p=1.0,\n",
        "            )\n",
        "            response_payload[\"generation_time_s\"] += round(time.time() - start_time_final_refine, 2)\n",
        "            response_payload[\"llm_full_response_obj_str\"] = str(final_openai_response)\n",
        "\n",
        "            if final_openai_response.choices and final_openai_response.choices[0].message:\n",
        "                refined_answer_text = final_openai_response.choices[0].message.content.strip()\n",
        "                response_payload[\"answer_text\"] = refined_answer_text + f\"\\n\\n(This answer is based on a summary of {doc_id_for_prompt}.)\"\n",
        "                logger.info(\"Summary refined by main LLM.\")\n",
        "                response_payload[\"parsed_citations\"] = []\n",
        "            else:\n",
        "                logger.error(\"OpenAI summary refinement response did not contain expected message content.\")\n",
        "                response_payload.update({\n",
        "                    \"answer_text\": \"Error: Could not parse text from AI's refined summary.\",\n",
        "                    \"error_message\": \"OpenAI summary refinement parsing error: No choices or message found.\"\n",
        "                })\n",
        "\n",
        "        except Exception as e_lc_summarize:\n",
        "            logger.error(f\"LangChain summarization process FAILED: {e_lc_summarize}. Falling back to standard generation.\", exc_info=True)\n",
        "            response_payload.update({\n",
        "                \"error_message\": f\"Summarization chain error: {str(e_lc_summarize)}. Fallback executed.\",\n",
        "                \"summarization_chain_used\": False\n",
        "            })\n",
        "            return generate_standard_llm_response(\n",
        "                user_query, formatted_context_from_rag, source_chunks_from_rag,\n",
        "                doc_id_for_prompt, dynamic_role_info, response_payload\n",
        "            )\n",
        "    else:\n",
        "        return generate_standard_llm_response(\n",
        "            user_query, formatted_context_from_rag, source_chunks_from_rag,\n",
        "            doc_id_for_prompt, dynamic_role_info, response_payload\n",
        "        )\n",
        "\n",
        "    response_payload.setdefault(\"answer_text\", \"Error: Summarization path failed to produce a final answer.\")\n",
        "    response_payload.setdefault(\"parsed_citations\", [])\n",
        "    return response_payload\n",
        "\n",
        "# --- One-time Initialization for AI/DB systems ---\n",
        "def perform_initial_setup():\n",
        "    logger.info(\"Flask App: Performing one-time initial setup for AI/DB systems...\")\n",
        "    if not AZURE_LLM_API_KEY_SECRET:\n",
        "        logger.critical(\"AZURE_LLM_API_KEY_SECRET MISSING. APP CANNOT FUNCTION.\")\n",
        "        return False\n",
        "\n",
        "    all_ok = True\n",
        "    if not initialize_rag_models_from_kb():\n",
        "        all_ok = False\n",
        "    if not initialize_llm_model():\n",
        "        all_ok = False\n",
        "    if not initialize_chromadb_from_kb():\n",
        "        all_ok = False\n",
        "    if HF_API_TOKEN_SECRET and not initialize_hf_client_from_parser():\n",
        "        logger.warning(\"HF Vision client (for parser) initialization failed or was skipped. Vision features might be limited.\")\n",
        "\n",
        "    if all_ok:\n",
        "        logger.success(\"Flask App: All critical AI/DB systems initialized.\")\n",
        "    else:\n",
        "        logger.error(\"Flask App: CRITICAL - One or more essential AI/DB system initializations FAILED.\")\n",
        "    return all_ok\n",
        "\n",
        "APP_SYSTEMS_READY = perform_initial_setup()\n",
        "\n",
        "# --- 12. Flask App Definition and Endpoints ---\n",
        "BASE_DIR = os.getcwd()\n",
        "TEMPLATES_DIR = os.path.join(BASE_DIR, 'templates')\n",
        "STATIC_DIR = os.path.join(BASE_DIR, 'static')\n",
        "STAGED_UPLOADS_DIR = os.path.join(BASE_DIR, 'colab_staged_uploads_final_nocite')\n",
        "\n",
        "os.makedirs(TEMPLATES_DIR, exist_ok=True)\n",
        "os.makedirs(STATIC_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(STATIC_DIR, 'js'), exist_ok=True)\n",
        "os.makedirs(STAGED_UPLOADS_DIR, exist_ok=True)\n",
        "\n",
        "app = Flask(__name__, template_folder=TEMPLATES_DIR, static_folder=STATIC_DIR)\n",
        "CORS(app)\n",
        "\n",
        "# --- HTML Templates (Placeholder - create these files) ---\n",
        "\n",
        "@app.route('/')\n",
        "def hero_page_route():\n",
        "    hero_html_path = os.path.join(TEMPLATES_DIR, 'hero-geometric.html')\n",
        "    if not os.path.exists(hero_html_path):\n",
        "        with open(hero_html_path, 'w') as f:\n",
        "            f.write('<!DOCTYPE html><html><head><title>DocuMind AI</title><style>body{font-family: sans-serif; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; background-color: #f0f2f5;} h1{color: #333;} a{text-decoration: none; padding: 10px 20px; background-color: #007bff; color: white; border-radius: 5px;}</style></head><body><h1>Welcome to DocuMind AI</h1><p>Your intelligent document assistant.</p><a href=\"/chatui\">Start Chatting</a></body></html>')\n",
        "    return render_template('hero-geometric.html')\n",
        "\n",
        "@app.route('/chatui')\n",
        "def chat_page_route():\n",
        "    chat_html_path = os.path.join(TEMPLATES_DIR, 'chat.html')\n",
        "    if not os.path.exists(chat_html_path):\n",
        "        with open(chat_html_path, 'w') as f:\n",
        "            f.write('<!DOCTYPE html><html><head><title>DocuMind AI Chat</title><style>body{font-family: sans-serif; padding: 20px; background-color: #f0f2f5;} h1{color: #333; text-align: center;}</style></head><body><h1>Chat with Your Documents</h1><div><!-- Basic chat UI elements would be added here by frontend JS --> <p>Chat interface placeholder. Real UI would be built with JavaScript.</p> <a href=\"/\">Back to Home</a></div></body></html>')\n",
        "    return render_template('chat.html')\n",
        "\n",
        "@app.route('/stage_file', methods=['POST'])\n",
        "def stage_single_file_route():\n",
        "    logger.info(\"--- API Call: /stage_file ---\")\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No 'file' part in the request.\"}), 400\n",
        "\n",
        "    file_obj = request.files['file']\n",
        "    original_filename = request.form.get('originalFilename', file_obj.filename)\n",
        "\n",
        "    if not file_obj or not original_filename:\n",
        "        return jsonify({\"error\": \"No file provided or filename is missing.\"}), 400\n",
        "\n",
        "    safe_original_filename = re.sub(r'[^a-zA-Z0-9._-]', '_', original_filename)\n",
        "\n",
        "    staging_id = f\"staged_{uuid.uuid4().hex[:8]}_{safe_original_filename}\"\n",
        "    staged_file_path = os.path.join(STAGED_UPLOADS_DIR, staging_id)\n",
        "\n",
        "    try:\n",
        "        file_obj.save(staged_file_path)\n",
        "        file_size = os.path.getsize(staged_file_path)\n",
        "        APP_WEB_STATE[\"staged_files\"][staging_id] = {\n",
        "            \"original_filename\": original_filename,\n",
        "            \"staged_path\": staged_file_path,\n",
        "            \"status\": \"staged\",\n",
        "            \"size\": file_size\n",
        "        }\n",
        "        logger.info(f\"  Staged '{original_filename}' (ID:{staging_id}, Size:{file_size}B) to '{staged_file_path}'.\")\n",
        "        return jsonify({\n",
        "            \"message\": \"File staged successfully.\",\n",
        "            \"original_filename\": original_filename,\n",
        "            \"staging_id\": staging_id,\n",
        "            \"status\": \"staged\"\n",
        "        }), 200\n",
        "    except Exception as e:\n",
        "        logger.error(f\"  Error staging '{original_filename}': {e}\", exc_info=True)\n",
        "        return jsonify({\"error\": f\"File staging failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/process_staged_files', methods=['POST'])\n",
        "def process_staged_files_route():\n",
        "    logger.info(\"--- API Call: /process_staged_files ---\")\n",
        "    if not APP_SYSTEMS_READY:\n",
        "        logger.error(\"Backend AI/DB systems are not ready. Cannot process files.\")\n",
        "        return jsonify({\"error\": \"Backend systems are not ready. Please try again later.\"}), 503\n",
        "\n",
        "    data = request.get_json()\n",
        "    if not data:\n",
        "        logger.warning(\"/process_staged_files: No JSON data received.\")\n",
        "        return jsonify({\"error\": \"Invalid JSON payload.\"}), 400\n",
        "\n",
        "    staging_ids = data.get('staging_ids', [])\n",
        "    options = data.get('options', {})\n",
        "    logger.info(f\"  Processing options: {options}. Staging IDs: {staging_ids}\")\n",
        "\n",
        "    processed_docs_summary = []\n",
        "    errors_during_processing = []\n",
        "\n",
        "    for staging_id in staging_ids:\n",
        "        staged_info = APP_WEB_STATE[\"staged_files\"].get(staging_id)\n",
        "        if not staged_info or staged_info.get(\"status\") != \"staged\":\n",
        "            err_msg = f\"File with staging ID '{staging_id}' not found or not in 'staged' state.\"\n",
        "            errors_during_processing.append({\"staging_id\": staging_id, \"filename\": staged_info.get(\"original_filename\", \"Unknown\"), \"error\": err_msg})\n",
        "            logger.warning(f\"  {err_msg}\")\n",
        "            continue\n",
        "\n",
        "        path_to_staged_file = staged_info[\"staged_path\"]\n",
        "        actual_original_filename = staged_info[\"original_filename\"]\n",
        "        logger.info(f\"  Processing '{actual_original_filename}' (ID: {staging_id}) from '{path_to_staged_file}'\")\n",
        "\n",
        "        profile_to_use = \"fastest\"\n",
        "        if options.get(\"handwritten\", False) or options.get(\"images\", False):\n",
        "            profile_to_use = \"comprehensive_vision\" if APP_STATE.get(\"vision_client_initialized\") else \"digital_plus_ocr\"\n",
        "        elif options.get(\"ocr\", False):\n",
        "            profile_to_use = \"digital_plus_ocr\"\n",
        "        elif not options:\n",
        "            profile_to_use = \"default_fallback\"\n",
        "\n",
        "        logger.info(f\"    Effective parsing profile for '{actual_original_filename}': '{profile_to_use}'\")\n",
        "\n",
        "        try:\n",
        "            parsed_blocks = parse_document_via_profile(path_to_staged_file, profile_name=profile_to_use)\n",
        "\n",
        "            if parsed_blocks and parsed_blocks[0].get(\"type\") == \"error\":\n",
        "                err_content = parsed_blocks[0].get('content', 'Unknown parsing error')\n",
        "                errors_during_processing.append({\"staging_id\": staging_id, \"filename\": actual_original_filename, \"error\": f\"Parsing/Conversion failed: {err_content}\"})\n",
        "                staged_info[\"status\"] = \"parse_failure\"\n",
        "                logger.error(f\"    Parsing/Conversion failed for '{actual_original_filename}': {err_content}\")\n",
        "                continue\n",
        "\n",
        "            base_collection_name = f\"{CHROMA_COLLECTION_NAME_PREFIX}_{os.path.splitext(actual_original_filename)[0]}_{uuid.uuid4().hex[:6]}\"\n",
        "            final_collection_name = sanitize_chromadb_collection_name(base_collection_name)\n",
        "\n",
        "            target_collection_obj = get_or_create_collection_cached(final_collection_name)\n",
        "            if not target_collection_obj:\n",
        "                err_msg = f\"Failed to get or create ChromaDB collection '{final_collection_name}'.\"\n",
        "                errors_during_processing.append({\"staging_id\": staging_id, \"filename\": actual_original_filename, \"error\": err_msg})\n",
        "                staged_info[\"status\"] = \"db_collection_failure\"\n",
        "                logger.error(f\"    {err_msg} for '{actual_original_filename}'\")\n",
        "                continue\n",
        "\n",
        "            ingestion_successful = ingest_document_into_knowledge_base(\n",
        "                parsed_blocks, final_collection_name, actual_original_filename, target_collection_obj\n",
        "            )\n",
        "\n",
        "            if ingestion_successful:\n",
        "                APP_WEB_STATE[\"processed_collections\"][actual_original_filename] = final_collection_name\n",
        "                staged_info.update({\"status\": \"processed\", \"collection_name\": final_collection_name})\n",
        "                processed_docs_summary.append({\n",
        "                    \"filename\": actual_original_filename,\n",
        "                    \"collection_name\": final_collection_name,\n",
        "                    \"staging_id\": staging_id,\n",
        "                    \"status\": \"processed\"\n",
        "                })\n",
        "                logger.info(f\"    Successfully ingested '{actual_original_filename}' into collection '{final_collection_name}'.\")\n",
        "                try:\n",
        "                    os.remove(path_to_staged_file)\n",
        "                    logger.info(f\"    Removed staged file '{path_to_staged_file}'.\")\n",
        "                    if staging_id in APP_WEB_STATE[\"staged_files\"]:\n",
        "                         del APP_WEB_STATE[\"staged_files\"][staging_id]\n",
        "\n",
        "                except Exception as e_remove:\n",
        "                    logger.warning(f\"    Could not remove staged file '{path_to_staged_file}': {e_remove}\")\n",
        "            else:\n",
        "                errors_during_processing.append({\"staging_id\": staging_id, \"filename\": actual_original_filename, \"error\": \"Ingestion into knowledge base failed.\"})\n",
        "                staged_info[\"status\"] = \"ingestion_failure\"\n",
        "                logger.error(f\"    Ingestion failed for '{actual_original_filename}' into '{final_collection_name}'.\")\n",
        "\n",
        "        except Exception as e_pipeline:\n",
        "            error_message = f\"Unhandled error in processing pipeline for '{actual_original_filename}': {str(e_pipeline)}\"\n",
        "            errors_during_processing.append({\"staging_id\": staging_id, \"filename\": actual_original_filename, \"error\": error_message})\n",
        "            if staged_info: staged_info[\"status\"] = \"pipeline_error\"\n",
        "            logger.error(f\"    {error_message}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"--- /process_staged_files call ended. Successfully processed: {len(processed_docs_summary)}, Errors: {len(errors_during_processing)} ---\")\n",
        "\n",
        "    if not processed_docs_summary and errors_during_processing:\n",
        "        return jsonify({\"error\": \"All file processing attempts failed.\", \"details\": errors_during_processing}), 500\n",
        "\n",
        "    return jsonify({\n",
        "        \"message\": \"File processing finished.\",\n",
        "        \"processed_documents\": processed_docs_summary,\n",
        "        \"errors\": errors_during_processing\n",
        "    }), 200\n",
        "\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask_question_route():\n",
        "    logger.info(\"--- API Call: /ask ---\")\n",
        "    if not APP_SYSTEMS_READY:\n",
        "        logger.error(\"Backend AI/DB systems not ready for /ask call.\")\n",
        "        return jsonify({\"error\": \"Backend systems are not ready. Please try again later.\"}), 503\n",
        "\n",
        "    data = request.get_json()\n",
        "    if not data or 'query' not in data:\n",
        "        logger.warning(\"/ask: Missing 'query' in JSON payload.\")\n",
        "        return jsonify({\"error\": \"Missing 'query' in request payload.\"}), 400\n",
        "\n",
        "    user_query = data['query']\n",
        "    logger.info(f\"  Received query for /ask: '{user_query[:100]}...'\")\n",
        "\n",
        "    active_document_filenames = data.get('active_documents', [])\n",
        "    collections_to_query_names = []\n",
        "    query_scope_description = \"all processed documents\"\n",
        "\n",
        "    if active_document_filenames:\n",
        "        logger.info(f\"  Frontend specified active documents: {active_document_filenames}\")\n",
        "        valid_collections_for_active_docs = []\n",
        "        all_active_docs_found = True\n",
        "        for fname in active_document_filenames:\n",
        "            coll_name = APP_WEB_STATE[\"processed_collections\"].get(fname)\n",
        "            if coll_name:\n",
        "                valid_collections_for_active_docs.append(coll_name)\n",
        "            else:\n",
        "                logger.warning(f\"    Document '{fname}' specified as active, but not found in processed collections. Query will default to all processed documents.\")\n",
        "                all_active_docs_found = False\n",
        "                break\n",
        "\n",
        "        if all_active_docs_found and valid_collections_for_active_docs:\n",
        "            collections_to_query_names = valid_collections_for_active_docs\n",
        "            if len(active_document_filenames) == 1:\n",
        "                query_scope_description = f\"document '{active_document_filenames[0]}'\"\n",
        "            else:\n",
        "                query_scope_description = f\"documents: {', '.join(active_document_filenames)}\"\n",
        "            logger.info(f\"  Query will target specific collections: {collections_to_query_names}\")\n",
        "        else:\n",
        "            collections_to_query_names = list(APP_WEB_STATE[\"processed_collections\"].values())\n",
        "            logger.info(f\"  Fallback: Querying ALL {len(collections_to_query_names)} processed collections.\")\n",
        "    else:\n",
        "        collections_to_query_names = list(APP_WEB_STATE[\"processed_collections\"].values())\n",
        "        logger.info(f\"  No specific documents indicated by frontend. Querying ALL {len(collections_to_query_names)} processed collections.\")\n",
        "\n",
        "    if not collections_to_query_names:\n",
        "        if not APP_WEB_STATE[\"processed_collections\"]:\n",
        "            return jsonify({\"answer_text\": \"No documents have been processed yet. Please upload and process a document first.\", \"parsed_citations\": []}), 400\n",
        "        else:\n",
        "            logger.error(\"Internal state error: No collections to query despite having processed documents, or collections list became empty.\")\n",
        "            return jsonify({\"answer_text\": \"Error: No valid document collections available to query at this time.\", \"parsed_citations\": []}), 500\n",
        "\n",
        "    is_aboutness_type_query = any(\n",
        "        phrase in user_query.lower() for phrase in [\"about this document\", \"summarize\", \"overview\", \"main idea\", \"tell me about\"]\n",
        "    )\n",
        "    if is_aboutness_type_query:\n",
        "        logger.info(\"  Query detected as an 'aboutness' or summarization request.\")\n",
        "\n",
        "    retrieval_results = retrieve_relevant_context(\n",
        "        user_query,\n",
        "        collections_to_query_names,\n",
        "        use_reranking=(APP_STATE.get(\"reranker_model\") is not None)\n",
        "    )\n",
        "\n",
        "    formatted_context = retrieval_results.get(\"formatted_context_for_llm\", \"\")\n",
        "    source_chunks = retrieval_results.get(\"source_chunks_retrieved\", [])\n",
        "\n",
        "    if \"Error:\" in formatted_context or not source_chunks:\n",
        "        logger.warning(f\"  Context retrieval yielded no usable chunks or an error: {formatted_context[:200]}\")\n",
        "\n",
        "    llm_generation_output = generate_llm_response(\n",
        "        user_query,\n",
        "        formatted_context,\n",
        "        source_chunks,\n",
        "        default_doc_name=query_scope_description,\n",
        "        is_aboutness_query_flag=is_aboutness_type_query\n",
        "    )\n",
        "\n",
        "    final_answer_text = llm_generation_output.get(\"answer_text\", \"Error: AI response generation failed or produced no text.\")\n",
        "    error_message_from_llm = llm_generation_output.get(\"error_message\")\n",
        "\n",
        "    if error_message_from_llm and \"Error:\" in final_answer_text:\n",
        "        logger.warning(f\"  LLM generation resulted in an error message displayed as answer: '{final_answer_text}'. Underlying error: {error_message_from_llm}\")\n",
        "    elif final_answer_text == AI_NO_ANSWER_POLICY or not final_answer_text.strip() or \"Error: AI\" in final_answer_text:\n",
        "        final_answer_text = \"I'm sorry, I couldn't find specific information to answer your question in the provided document(s).\"\n",
        "        logger.info(\"  AI indicated no answer or returned an empty/error state; providing a polite 'no information' message.\")\n",
        "\n",
        "    logger.info(f\"--- /ask call finished. Answer length: {len(final_answer_text)}. Summarization used: {llm_generation_output.get('summarization_chain_used', False)} ---\")\n",
        "\n",
        "    return jsonify({\n",
        "        \"answer_text\": final_answer_text,\n",
        "        \"parsed_citations\": llm_generation_output.get(\"parsed_citations\", []),\n",
        "        \"llm_prompt_sent\": \"REDACTED_IN_RESPONSE\",\n",
        "        \"error_message\": error_message_from_llm,\n",
        "        \"summarization_chain_used\": llm_generation_output.get(\"summarization_chain_used\", False)\n",
        "    })\n",
        "\n",
        "@app.route('/list_processed_documents_for_chat', methods=['GET'])\n",
        "def list_processed_docs_for_chat_api():\n",
        "    processed_filenames = list(APP_WEB_STATE[\"processed_collections\"].keys())\n",
        "    logger.info(f\"API Call: /list_processed_documents_for_chat. Returning {len(processed_filenames)} document filenames.\")\n",
        "    return jsonify({\"processed_document_filenames\": processed_filenames}), 200\n",
        "\n",
        "# --- Ngrok and App Run Function ---\n",
        "public_url_cell4_final = None\n",
        "\n",
        "def start_flask_app_cell4_final():\n",
        "    global public_url_cell4_final\n",
        "    if public_url_cell4_final:\n",
        "        try:\n",
        "            ngrok.disconnect(public_url_cell4_final)\n",
        "            logger.info(\"Disconnected previous ngrok tunnel.\")\n",
        "        except Exception as e_ngrok_disc:\n",
        "            logger.warning(f\"Could not disconnect previous ngrok tunnel: {e_ngrok_disc}\")\n",
        "        public_url_cell4_final = None\n",
        "\n",
        "    try:\n",
        "        NGROK_AUTHTOKEN_SECRET = userdata.get('NGROK_AUTHTOKEN')\n",
        "        if NGROK_AUTHTOKEN_SECRET:\n",
        "            conf.get_default().auth_token = NGROK_AUTHTOKEN_SECRET\n",
        "            logger.info(\"Ngrok authentication token set successfully.\")\n",
        "        else:\n",
        "            logger.warning(\"NGROK_AUTHTOKEN not found in Colab secrets. Ngrok might be rate-limited or fail for non-authenticated users.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        logger.warning(\"NGROK_AUTHTOKEN secret not found. Proceeding without ngrok authentication.\")\n",
        "    except Exception as e_ngrok_auth:\n",
        "        logger.warning(f\"An error occurred while setting ngrok authtoken: {e_ngrok_auth}\")\n",
        "\n",
        "    if not APP_SYSTEMS_READY:\n",
        "        logger.critical(\"CRITICAL: Backend AI/DB systems FAILED initialization. Flask application will not run effectively or may fail.\")\n",
        "        print(\"Flask app cannot start because critical backend systems are not ready. Check logs for errors.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        public_url_cell4_final = ngrok.connect(5000)\n",
        "        logger.info(f\"Flask application (Merged FINAL Version - NoCite) is accessible at: {public_url_cell4_final}\")\n",
        "        print(f\" * Ngrok tunnel (FINAL NoCite Version) \\\"{public_url_cell4_final}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "\n",
        "        app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "    except Exception as e_flask_run:\n",
        "        logger.error(f\"Error starting Flask application or ngrok tunnel (FINAL NoCite Version): {e_flask_run}\", exc_info=True)\n",
        "        if public_url_cell4_final:\n",
        "            try:\n",
        "                ngrok.disconnect(public_url_cell4_final)\n",
        "            except Exception as e_disc_fatal:\n",
        "                logger.error(f\"Failed to disconnect ngrok tunnel during error handling: {e_disc_fatal}\")\n",
        "        try:\n",
        "            ngrok.kill()\n",
        "        except Exception as e_kill_fatal:\n",
        "            logger.error(f\"Failed to kill ngrok processes during error handling: {e_kill_fatal}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "\n",
        "start_flask_app_cell4_final()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
